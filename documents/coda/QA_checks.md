This page described the QA checks that we want to perform when using the VarEmb model to make inferences on a new dataset. They are not related to the model training, which is a separate process.

These are the separate stages in which we wanna perform checks:

- **Input** - Checks in the input data we will use for the inference.
- **Preprocessing** - Checks for the data transformations before calling the model.
- **Predictions** - Checks in the outputs of the model.
- **Sizing** - Checks for the sizing process.
- **Post** **processing** - Checks for the final file after the predictions were post processed.



# Input

These are the datasets used as input to the process:

## Input datasets
| Name | Example of path | Needs check |
| --- | --- | --- |
| Brand total counts | s3://226109243659-vzw-data-store/lake/refreshes/refresh_{refresh_date_path}/brand_total_counts.delta/ | No (already checked by the refresh pipeline) |
| Brand tfidf | s3://226109243659-vzw-data-store/lake/refreshes/refresh_{refresh_date_path}/brand_tfidf.delta/ | No (already checked by the refresh pipeline) |
| Brand titles | s3://226109243659-vzw-data-store/lake/refreshes/refresh_{refresh_date_path}/reference_data/brand_titles.csv | No (already checked by the refresh pipeline) |
| Brand blocklist | s3://226109243659-vzw-data-store/lake/refreshes/refresh_{refresh_date_path}/reference_data/brand_blocklist.csv | No (already checked by the refresh pipeline) |
| Brand allowlist | s3://226109243659-vzw-data-store/lake/reference-data/brand_allowlist_20240214.csv/ | Yes |
| Mapper file | s3://226109243659-vzw-data-science-longterm/talvany/ai_products/brand_prediction/mapper/refresh_20231125/all/001.delta | Yes |




## Modelled Brands QA Checks
| Script/Variable | Type | Description | Check Spec | Value/Impact (what is the worst that can happen if we don't check this) | Likelihood | Computational Cost | Execution stage | Root cause of Failure | Status (scoped, approved, implemented, tested, orchestrated) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Brand allow list | Data | Check schema of the allow list file. Schema of the file should be [title(str)]. | **IF** schema **==** [title(str)]<br/><br/>**PASS**<br/><br/>**ELSE**<br/><br/>**FAIL with message** "The schema of the brand allow list is wrong. It should be [title(str)], but it is {SCHEMA}. Please ask the MLE team to check that the file is correct." | If the schema of the file is not correct, the process will likely break. |  | Very low. <br/>Only schema is read and compared | Input | The file is corrupted. | Implemented, tested and Orchestated |
| Mapper data | Data | Check schema of the mapper file. Schema of the file should be [title(str), item_id(str)]. | **IF** schema **==** [title(str), item_id(str)]<br/><br/>**PASS**<br/><br/>**ELSE**<br/><br/>**FAIL with message** "The schema of the mapper file is wrong. It should be [title(str), item_id(str)], but it is {SCHEMA}. Please ask the MLE team to check that the file is correct." | If the schema of the file is not correct, the process will likely break. |  | Very low. <br/>Only schema is read and compared | Input | The file is corrupted. | Implemented, tested and Orchestated |
| Allowed brand list | Reference Data | Ensure that the allowed brand list is the same one from the last time. If it is not, we will issue a warning so that we can check that this was expected.  | Compare the list of brands of the allow list of the current run (A) and of the previous run (A_0).<br/><br/>**IF** A == A_0<br/><br/>**PASS**<br/><br/>**ELSE**<br/><br/>**WARNING** with message "The allow list is different from the previous run. Please check that <filename> contains the most up to date brand allowlist approved by Verizon legal and ensure that the VZ QA team are aware of this change in available brands." | The allowed brand list has direct impact on the final results, as we will only generate data for the brands in the allowed brand list. If there are changes to the file, this should be known and well communicated. |  | Very low. <br/>The action is done on the smaller data set | Input | The writing of the file could be corrupted.<br/>Duplicates exist. | Implemented, tested and Orchestated |
| Mapper data | Reference Data | Ensure # of mapped brands did not vary too much. | Calculate # of mapped brands in the current run (n) and in the previous run (n_0). # of brands should not have decreased at all, nor increased more than 10%.<br/><br/>**IF** n < n_0 **OR** n > (n_0 + 10%)<br/><br/>**FAIL with message** "Number of mapped brands not acceptable. Number of brands in the current run is {n}, while in the previous run it was {n_0}. Here is a sample of the brands lost {sample of 1% of the brands lost}. The mapper file could be corrupted. Please ask a data scientist to investigate."<br/><br/>**ELSE**<br/><br/>**PASS** | A small increase in the number of brand is expected. The average monthly increase is 1.14% (see [EDA](https://coda.io/d/_dK5h4iVEEUo/_suCOp#_lu1Es)). If we have a hard increase or a decrease in the number of brands, that might indicated that the mapper file or the original data is corrupted. If the mapper file is corrupted, that would compromise all predictions. |  | Low. <br/>Only current mapper data set count is read. The previous run count is stored in a json file and is read | Input | The writing of the mapper could be corrupted. | Implemented, tested and Orchestated |
| **User count in preprocessed data** | Data | Check total number of users which are in the preprocessed data.<br/>A loss of users during preprocessing is expected, as some users will end up without brands after the filtering.<br/>Past executions have seen a decrease of about 0.1% in the number of users.<br/>We will assume that up to 1% of lost users is normal. Between 1-10% we should investigate, but we can still proceed, as the final predictions will probably still have enough data. More than 10% we should stop the process. | Calculate # of unique users in the input (n_input) and in the preprocessed data (n_proc).<br/><br/>**IF** n_proc < (n_input - 10%)<br/><br/>**FAIL with message** "Number of unique users decreased more than 10%". Current count is  {n_input} , while before it was {n_proc}. There could be a problem with the pre processing. Here are the line ids of the users lost: {list of line_ids}. Please ask a data scientist to investigate."<br/><br/>**ELIF** n_proc < (n_input - 1%)<br/><br/>**WARNING** "Number of unique users decreased more than 1%". Current count is  {n_input} , while before it was {n_proc}. Here are the line ids of the users lost: {list of line_ids}."<br/><br/>**ELSE**<br/><br/>**PASS** | We expect up to 1% of users to be lost in the preprocessing and we estimate that we can tolerate up to 10% lost. However, if we lose too many users, we might have brands that will not be well represented due to lack of data. |  | Low. <br/>Only count is read from the input data set and preprocessed data set | Preprocessing | The preprocessing filtered out too many users. | Implemented, tested and Orchestated |
| Duplicates in predictions |  | Check if there are any duplicates in the data.  | For every <user, brand>, check how many scores we have. We only want a single score for any pair.<br/><br/>IF duplicates exist:<br/><br/>**WARNING with message** ("There are duplicates in the predictions. Here are the duplicates: {pairs and scores of the duplicates}."<br/><br/>**ELSE**<br/><br/>**PASS** | If duplicates exist in the score, we probably have a bug in the process. However, IE will probably only use one of the scores, so the impact is not crucial and we do not have to stop the process. |  | Low to medium. <br/>The duplicate check happens on 67M data set | Predictions |  | Implemented, tested and Orchestated |
| Score distribution in sized data |  | Check distribution of scores. | Get a list of all the scores in the sized data (S) and a list of all the scores in the previous run (S_0).<br/><br/>Perform a Kolmogorov-Smirnov test between those two lists/distributions. The resulting statistic should **not** be higher than 0.1<br/><br/>**IF** statistic < 0.1<br/><br/>**PASS**<br/><br/>**ELSE**<br/><br/>**FAIL with message** ("The score distribution in the sized data is not acceptable. The calculate statistic {statistic} is lower than 0.1. This could indicate a problem with the data or with the model. Here is a plot of the two distributions compared: {plot}. Ask a data scientist to investigate." | If the distribution of the scores changes significantly, the model is probably not providing predictions of the same level as last time. So the final results that the user sees will potentially not be of good quality.<br/><br/>We have a detailing explanation of why we chose that threshold in the [EDA page](https://coda.io/d/_dK5h4iVEEUo/_suCOp#_luczh). |  | Medium<br/>Kolmogorov-Smirnov test is performed on the current data set. The previous stats are read from json file | Sized data | The model is not generating the scores correctly. | Implemented, tested and Orchestated |
| Brands per user - overall - post processed data |  | Check the overall counts of brands per user  in the post-processed data. | Calculate the # of brands for each user in the current run. Get the mean (M) and standard deviation (SD) between all of those numbers.<br/><br/>Calculate the # of brands for each user in the previous run. Get the mean (M_0) and standard deviation (SD_0) between all of those numbers.<br/><br/>The new mean and standard deviation should not deviate more than 25% from the previous ones.<br/><br/>**IF** \|M_0 - M\| > (M x 0.25) **OR** \|S_0 - S\| > (S x 0.25)<br/><br/>**WARNING with message** "The mean and/or standard deviation of the new run deviated more than 25% from the previous run. In the current run we have mean {M} and standard deviation {SD}, while in the previous run we had mean {M_0} and standard deviation {SD_0}"<br/><br/>**ELSE**<br/><br/>**PASS** | If the overall number of brands predicted per user changes significantly, the quality of the final predictions could be affected. So we should analyse this, but this does not have to stop the process. |  | Low to medium<br/>Mean and Median of the current process are generated and saved in a json file as part of Preprocessing.<br/>For QA check, only these stored stats from json are read. Also the previous stats are in json | Preprocessing |  | Implemented, tested and Orchestated |
| Brands per user - extremes - post processed data |  | Check the counts of brands per user volume in the extremes (too many brands, too few brands) in the post-processed data. | Calculate the # of brands for each user in the current run.<br/><br/>Count how many users have more than 200 brands (high_count).<br/><br/>Count how many users have more fewer than 5 brands (low_count).<br/><br/>**IF** high_count > 2% of total users OR low_count > 2% of total users<br/><br/>**WARNING with message** "Too many users (more than 2%) with either too many (>200) or too few brands (<5) brands. Number of users with too many brands: {high_count}. Number of users with too few brands: {low_count}. Total number of users: {total users}."<br/><br/>**ELIF** high_count > 5% of total users OR low_count > 5% of total users<br/><br/>**FAIL with message** "Too many users (more than 5%) with either too many (>200) or too few brands (<5) brands. Number of users with too many brands: {high_count}. Number of users with too few brands: {low_count}. Total number of users: {total users}. This could indicate a problem with the model or the data. Ask a data scientist to investigate."<br/><br/>**ELSE**<br/><br/>**PASS** | Users with too many or too few brands are not very useful. So if we see an increase in either of those cases, that might indicated that out final predictions will not be of the same quality of before. |  | High<br/>The computation happnens on the full data set | Preprocessing |  | Implemented, tested and Orchestated |
| Users per brand - overall - post processed data |  | Check overall users per brand volume (# of users per brand) in the post processed data. <br/>This should not be less than the minimum nor more than the maximum count we define below:<br/>Minimum: the minimum number of users we can guarantee with the sizing process (1000) minus torelance for RTBF (~100). So minimum is 900.<br/>Maximum - the maximum number of users, i.e. around 67M. | Calculate the total # of users in the post processed data.<br/><br/>Calculate the # of unique users for each brand (count_unique).<br/><br/>**IF** **ANY** count_unique is > total # of users<br/><br/>**FAIL** with message "There are brands more users than the maximum number allowed (the maximum is the total number of unique users: {total}). Here are those brands and their corresponding counts of users: {print the names of the brands that did not comply and the counts}. Ask a data scientist to investigate.<br/><br/>**ELIF** count is < 900 in more than 10 brands<br/><br/>**FAIL** with message "There are brands fewer users than the minimum number allowed (minimum is 900). Here are those brands and their corresponding counts of users: {print the names of the brands that did not comply and the counts}. This could potentially be a problem with the sizing step, which could compromise the final results. Ask a data scientist to investigate"<br/><br/>**ELSE**<br/><br/>**PASS** | If we do not provide enough users for brands, searches for those brands will not return enough results.<br/><br/>*We allow up to 10 brands to fail the minimum count as some brands don't have enough data and don't have more than a few users.*<br/><br/>*Also, even though the sizing process guarantees a minimum of 1000 users per brand, we set 900 as the minimum. That is because we lose users from the predictions when they get excluded by the RTBF process.* |  | High<br/>The computation happnens on the full data set | Post processed data |  | Implemented, tested and Orchestated |
| Number of users - post processed data |  | # of users should be acceptable.<br/><br/>If the # is correct in the final file, it means it was correct in the previous stages too. | The # of unique users in the post processed data (N_POS) should correspond to # of unique users in the preprocessed data (N_PRE).<br/><br/>**IF** N_PRE == N_POS<br/><br/>**PASS**<br/><br/>**ELIF** \|N_POS - N_PRE\| < 1M<br/><br/>**WARNING with message** "The number of users in the post processed data does not correspond to the number of users in the preprocessed data. The difference is {N_POS - N_PRE}, and here is a sample of those users: {print sample of 1k missing users}"<br/><br/>**ELSE**<br/><br/>**FAIL with message** "The number of users in the post processed data does not correspond to the number of users in the preprocessed data and the difference between those is higher than 1M. The difference is {N_POS - N_PRE}, and here is a sample of those users: {print sample of 1k missing users}. This could impact the data volume provided in the final output. Ask a data scientist to investigate."<br/><br/>RTBF should be taken into account here, such that users whose data was deleted should not be part of the calculation. | Previous runs had exact numbers of users, so we should be warned if that's not the case as it's probably a bug. However, as long as we did not lose too many users, the brand predictions will still be of expected quality. |  | Low to medium<br/>Here, we take unique users count for final data compared to preprocessed data | Post processed data |  | Implemented, tested and Orchestated |
| Number of brands - post processed data |  | # of brands in the post processed data should be acceptable | Calculate the # of unique brands in the allow list (N_A) and the # of unique brands in the post processed data (N_POS). We should keep keep all brands, with a tolerance for missing up to 20 brands.<br/><br/>**IF** N_A == N_POS<br/><br/>**PASS**<br/><br/>**ELIF** (N_A - N_PRE) < 20<br/><br/>**WARNING with message** ("A total of {N_A - N_PRE} brands from the allow list are present in the post processed data. Here are the missing brands: {print missing brands} "<br/><br/>**ELSE**<br/><br/>**FAIL with message** ("A total of {N_A - N_PRE} brands from the allow list are present in the post processed data. That number is higher than the tolerance of 20. That will cause a high number of expected brands not to appear in the final predictions. Here are the missing brands: {print missing brands}. Ask a data scientist to investigate." | Any brands that we lose here will not be available in the output file.  |  | Low to medium<br/>Here, we take unique brands count for final data compared to allow list brands | Post processed data |  | Implemented, tested and Orchestated |
| Assessment metrics (relative overall) |  | Check precision, recall and f1-score. | Calculate the **overall** median assessment metrics of Precision, Recall and F1-score in a 1M user sample of the new data (only allowed brands).<br/><br/>Obtain the same metrics for the past run.<br/><br/>No metrics should have a relative difference of **less than 5%** from the previous run. For instance, if the brand precision in the last run was 20%, the new brand precision should not be lower than 19%.<br/><br/><br/><br/>**IF** **ANY** metric < (old_metric - 5%) <br/><br/>**FAIL with message** "Some of the overall assessment metrics diverged more than 5% from the previous metrics. This is a strong indicative that the model is not performing as it should. Here are all the from the current and the previous run {print all overall metrics from both runs}. Ask a data scientist to investigate."<br/><br/>**ELSE**<br/><br/>**PASS** | The assessment metrics are the best indication of the quality of the model. If their values drop in relation to the previous run, that means that the quality of the predictions will drop as well. <br/><br/>For more details, see [EDA](https://coda.io/d/_dK5h4iVEEUo/_suCOp#_lu1Es). |  | High<br/>The computation happnens on the full data set | Post processed data |  | Implemented, tested and Orchestated |
| Assessment metrics (absolute overall) |  | Check precision, recall and f1-score.<br/><br/> | Calculate the median assessment metrics of Precision, Recall and F1-score in a 1M user sample of the new data (only allowed brands).<br/><br/>No metrics should be lower than their absolute threshold value (see tables below). For instance, if the brand precision is 4.6%, it should issue a warning (4.6% < 4.7%).<br/><br/><br/><br/>**IF** **ANY** metric < corresponding warning threshold <br/><br/>**WARNING with message** "Some of the overall assessment metrics were less than the absolute thresholds previously defined for a WARNING. Here are the metrics and the thresholds: {print all metrics and all WARNING thresholds}."<br/><br/>**ELIF ANY** metric < corresponding fail threshold<br/><br/>**FAIL with message** "Some of the overall assessment metrics were less than the absolute thresholds previously defined for a FAIL. This is a strong indicative that the model is not performing as it should. Here are the metrics and the thresholds: {print all metrics and all FAIL thresholds}. Ask a data scientist to investigate."<br/><br/>**ELSE**<br/><br/>**PASS** | The assessment metrics are the best indication of the quality of the model. If their values drop below a minimum value, that means that the quality of the predictions will not be of good quality. <br/><br/>For more details, see [EDA](https://coda.io/d/_dK5h4iVEEUo/_suCOp#_lu1Es). |  | High<br/>The computation happnens on the full data set | Post processed data |  | Implemented, tested and Orchestated |
| Assessment metrics (relative per brand) |  | Check precision, recall and f1-score. | Calculate the **per brand** assessment metrics of Precision, Recall and F1-score in a 1M user sample of the new data (only allowed brands).<br/><br/>Obtain the same metrics for the past run.<br/><br/>No metrics from any brands should have a relative difference of **less than 20%** from the previous run. For instance, if the precision for Adidas in the last run was 20%, the new brand precision should not be lower than 16%.<br/><br/><br/><br/>**IF** **ANY** metric < (old_metric - 20%) <br/><br/>**WARNING with message** "The metrics of some brands decreased more than the limit of 20%. Here are the affected brands and metrics: {print affected brands and metrics} "<br/><br/>**ELSE**<br/><br/>**PASS** | The assessment metrics are the best indication of the quality of the model. If their values drop in relation to the previous run, that means that the quality of the predictions will drop as well.  |  | High<br/>The computation happnens on the full data set | Post processed data |  | Implemented, tested and Orchestated |
| Assessment metrics (absolute per brand) |  | Check precision, recall and f1-score. | Calculate the median **per brand** assessment metrics of Precision, Recall and F1-score in a 1M user sample of the new data (only allowed brands).<br/><br/>Alert of brands with low scores.<br/><br/>**IF** **ANY** metric < 0.01<br/><br/>**WARNING with message** "The metrics of some brands are lower than the threshold of 0.01. Here are the affected brands and metrics: {print affected brands and metrics} "<br/><br/>**ELSE**<br/><br/>**PASS** | The assessment metrics are the best indication of the quality of the model. If their values are too low, the brand results might not be reliable. |  | High<br/>The computation happnens on the full data set | Post processed data |  | Implemented, tested and Orchestated |


## Assessment absolute thresholds









 





#