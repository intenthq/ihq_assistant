{
    "title": "AIPROD-1864 Add audit columns and triggers to graph",
    "description": null,
    "author": "cdagraca",
    "created_at": "2025-01-27T16:30:13Z",
    "changed_files": [
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_blocklist_views.sql",
            "patch": "@@ -1,52 +0,0 @@\n-SET SEARCH_PATH = combined_graph;\n-\n-CREATE OR REPLACE VIEW content_categories AS\n-    SELECT COALESCE(c.content_type, i.content_type) AS content_type,\n-           COALESCE(c.description, i.description) AS description,\n-           COALESCE(c.block_level, i.block_level) AS block_level,\n-           LEAST(c.total_block_level, i.total_block_level) AS total_block_level\n-    FROM ihq_graph.content_categories i\n-    FULL OUTER JOIN client_graph.content_categories c ON\n-        c.content_type = i.content_type;\n-\n-CREATE OR REPLACE VIEW ihq_topic_content_categories AS\n-    SELECT  COALESCE(c.topic_id, i.topic_id) AS topic_id,\n-            COALESCE(c.content_type, i.content_type) AS content_type,\n-            CASE\n-                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level AND\n-                     c.content_level IS NOT NULL\n-                THEN GREATEST(cc.total_block_level, c.content_level)\n-                ELSE COALESCE(c.content_level, i.content_level)\n-            END AS content_level,\n-            COALESCE(c.notes, i.notes) AS notes,\n-            CASE\n-                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level THEN False\n-                ELSE COALESCE(c.exclude, False)\n-            END AS exclude\n-    FROM ihq_graph.ihq_topic_content_categories i\n-    JOIN ihq_graph.content_categories cc ON\n-        i.content_type = cc.content_type\n-    FULL OUTER JOIN client_graph.ihq_topic_content_categories c ON\n-        c.topic_id = i.topic_id AND\n-        c.content_type = i.content_type;\n-\n-CREATE OR REPLACE VIEW named_entity_content_categories AS\n-    SELECT  COALESCE(c.entity_name, i.entity_name) AS entity_name,\n-            COALESCE(c.content_type, i.content_type) AS content_type,\n-            CASE\n-                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level AND\n-                     c.content_level IS NOT NULL\n-                THEN GREATEST(cc.total_block_level, c.content_level)\n-                ELSE COALESCE(c.content_level, i.content_level)\n-            END AS content_level,\n-            COALESCE(c.notes, i.notes) AS notes,\n-            CASE\n-                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level THEN False\n-                ELSE COALESCE(c.exclude, False)\n-            END AS exclude\n-    FROM ihq_graph.named_entity_content_categories i\n-    JOIN content_categories cc ON\n-        i.content_type = cc.content_type\n-    FULL OUTER JOIN client_graph.named_entity_content_categories c ON\n-        c.entity_name = i.entity_name AND\n-        c.content_type = i.content_type;\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_client_schema.sql",
            "patch": "@@ -3,97 +3,184 @@ CREATE SCHEMA IF NOT EXISTS client_graph;\n SET SEARCH_PATH = client_graph;\n \n CREATE TABLE IF NOT EXISTS novatiq_iab_categories (\n-   novatiq_id   VARCHAR(10) PRIMARY KEY,\n-   exclude      BOOLEAN NOT NULL,\n+    novatiq_id   VARCHAR(10) PRIMARY KEY,\n+    exclude      BOOLEAN     NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n    FOREIGN KEY (novatiq_id) REFERENCES ihq_graph.novatiq_iab_categories(novatiq_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topics (\n-   topic_id       VARCHAR(50) PRIMARY KEY,\n-   is_virtual     BOOLEAN,\n-   exclude        BOOLEAN NOT NULL,\n+    topic_id       VARCHAR(50) PRIMARY KEY,\n+    is_virtual     BOOLEAN,\n+    exclude        BOOLEAN     NOT NULL,\n+    created_by    VARCHAR(100) NOT NULL,\n+    created_at    TIMESTAMP    NOT NULL,\n+    modified_by   VARCHAR(100),\n+    modified_at   TIMESTAMP,\n    FOREIGN KEY (topic_id) REFERENCES ihq_graph.ihq_topics(topic_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topic_parents(\n-    topic_id    VARCHAR(50) NOT NULL,\n-    parent_id   VARCHAR(50) NOT NULL,\n-    exclude     BOOLEAN NOT NULL,\n+    topic_id    VARCHAR(50)  NOT NULL,\n+    parent_id   VARCHAR(50)  NOT NULL,\n+    exclude     BOOLEAN      NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (topic_id, parent_id),\n     FOREIGN KEY (topic_id) REFERENCES ihq_graph.ihq_topics(topic_id),\n     FOREIGN KEY (parent_id) REFERENCES ihq_graph.ihq_topics(topic_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topic_novatiq_iab_categories(\n-    topic_id    VARCHAR(50) NOT NULL,\n-    novatiq_id  VARCHAR(10) NOT NULL,\n-    exclude     BOOLEAN NOT NULL,\n+    topic_id    VARCHAR(50)  NOT NULL,\n+    novatiq_id  VARCHAR(10)  NOT NULL,\n+    exclude     BOOLEAN      NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (topic_id, novatiq_id),\n     FOREIGN KEY (topic_id) REFERENCES ihq_graph.ihq_topics(topic_id),\n     FOREIGN KEY (novatiq_id) REFERENCES ihq_graph.novatiq_iab_categories(novatiq_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topic_labels (\n-    topic_id VARCHAR(50)  NOT NULL,\n-    lang     VARCHAR(2)   NOT NULL,\n-    label    VARCHAR(255) NOT NULL,\n+    topic_id    VARCHAR(50)  NOT NULL,\n+    lang        VARCHAR(2)   NOT NULL,\n+    label       VARCHAR(255) NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (topic_id, lang),\n     FOREIGN KEY (topic_id) REFERENCES ihq_graph.ihq_topics(topic_id)\n );\n \n+-- No modified columns; changing domain should delete and create a new record; removing exclude should be a delete\n CREATE TABLE IF NOT EXISTS stemmed_domains (\n-    stemmed_domain VARCHAR(100) PRIMARY KEY,\n-    exclude        BOOLEAN NOT NULL\n+    stemmed_domain  VARCHAR(100) PRIMARY KEY,\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL\n );\n \n+-- No modified columns; changing domain or topic should delete and create a new record; removing exclude should be a delete\n CREATE TABLE IF NOT EXISTS stemmed_domain_ihq_topics (\n-    stemmed_domain VARCHAR(100) NOT NULL,\n-    topic_id       VARCHAR(50)  NOT NULL,\n-    exclude        BOOLEAN      NOT NULL,\n+    stemmed_domain  VARCHAR(100) NOT NULL,\n+    topic_id        VARCHAR(50)  NOT NULL,\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n     PRIMARY KEY (stemmed_domain, topic_id),\n     FOREIGN KEY (topic_id) REFERENCES ihq_graph.ihq_topics(topic_id)\n );\n \n CREATE INDEX IF NOT EXISTS stemmed_domain_ihq_topics_topic_id_ix ON stemmed_domain_ihq_topics(topic_id);\n \n CREATE TABLE IF NOT EXISTS named_entities (\n-    entity_name   VARCHAR(255) PRIMARY KEY,\n-    default_label VARCHAR(255) NOT NULL,\n-    exclude       BOOLEAN NOT NULL\n+    entity_name     VARCHAR(255) PRIMARY KEY,\n+    default_label   VARCHAR(255) NOT NULL,\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP\n );\n \n CREATE TABLE IF NOT EXISTS named_entity_labels (\n     entity_name VARCHAR(255) NOT NULL,\n     lang        VARCHAR(2)   NOT NULL,\n     label       VARCHAR(255) NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (entity_name, lang)\n );\n \n CREATE TABLE IF NOT EXISTS wiki_brands (\n     wiki_id INTEGER PRIMARY KEY,\n     default_label   VARCHAR(255) NOT NULL,\n     entity_name     VARCHAR(255),\n-    exclude         BOOLEAN NOT NULL,\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     FOREIGN KEY (wiki_id) REFERENCES ihq_graph.wiki_brands(wiki_id)\n );\n \n CREATE TABLE IF NOT EXISTS wiki_brand_labels (\n-    wiki_id INTEGER,\n-    lang    VARCHAR(2),\n-    label   VARCHAR(255) NOT NULL,\n+    wiki_id     INTEGER,\n+    lang        VARCHAR(2),\n+    label       VARCHAR(255) NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (wiki_id, lang)\n );\n \n+-- No modified columns; changing domain or entity should delete and create a new record; removing exclude should be a delete\n CREATE TABLE IF NOT EXISTS stemmed_domain_named_entities (\n-    stemmed_domain VARCHAR(100),\n-    entity_name    VARCHAR(255),\n-    exclude        BOOLEAN NOT NULL,\n+    stemmed_domain  VARCHAR(100),\n+    entity_name     VARCHAR(255),\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n     PRIMARY KEY (stemmed_domain, entity_name)\n );\n \n CREATE INDEX IF NOT EXISTS stemmed_domain_named_entities_entity_name_ix ON stemmed_domain_named_entities(entity_name);\n \n CREATE TABLE IF NOT EXISTS language_preferences (\n-    lang                VARCHAR(2),\n-    preference_order    INTEGER\n+    lang                VARCHAR(2)   PRIMARY KEY,\n+    preference_order    INTEGER      NOT NULL,\n+    created_by          VARCHAR(100) NOT NULL,\n+    created_at          TIMESTAMP    NOT NULL,\n+    modified_by         VARCHAR(100),\n+    modified_at         TIMESTAMP,\n+    UNIQUE (preference_order)\n+);\n+\n+CREATE TABLE IF NOT EXISTS content_categories (\n+    content_type        VARCHAR(50)  PRIMARY KEY,\n+    description         VARCHAR,\n+    block_level         INTEGER,\n+    total_block_level   INTEGER,\n+    created_by          VARCHAR(100) NOT NULL,\n+    created_at          TIMESTAMP    NOT NULL,\n+    modified_by         VARCHAR(100),\n+    modified_at         TIMESTAMP\n+);\n+\n+CREATE TABLE IF NOT EXISTS ihq_topic_content_categories (\n+    topic_id        VARCHAR(50)  NOT NULL,\n+    content_type    VARCHAR(50)  NOT NULL,\n+    content_level   INTEGER      NOT NULL,\n+    notes           VARCHAR,\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n+    PRIMARY KEY (topic_id, content_type)\n+);\n+\n+CREATE TABLE IF NOT EXISTS named_entity_content_categories (\n+    entity_name     VARCHAR(255) NOT NULL,\n+    content_type    VARCHAR(50)  NOT NULL,\n+    content_level   INTEGER,\n+    notes           VARCHAR,\n+    exclude         BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n+    PRIMARY KEY (entity_name, content_type)\n );"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_client_schema_blocklist.sql",
            "patch": "@@ -1,26 +0,0 @@\n-SET SEARCH_PATH = client_graph;\n-\n-CREATE TABLE IF NOT EXISTS content_categories (\n-    content_type        VARCHAR(50) PRIMARY KEY,\n-    description         VARCHAR,\n-    block_level         INTEGER,\n-    total_block_level   INTEGER\n-);\n-\n-CREATE TABLE IF NOT EXISTS ihq_topic_content_categories (\n-    topic_id        VARCHAR(50) NOT NULL,\n-    content_type    VARCHAR(50) NOT NULL,\n-    content_level   INTEGER NOT NULL,\n-    notes           VARCHAR,\n-    exclude         BOOLEAN NOT NULL,\n-    PRIMARY KEY (topic_id, content_type)\n-);\n-\n-CREATE TABLE IF NOT EXISTS named_entity_content_categories (\n-    entity_name     VARCHAR(255) NOT NULL,\n-    content_type    VARCHAR(50) NOT NULL,\n-    content_level   INTEGER,\n-    notes           VARCHAR,\n-    exclude         BOOLEAN NOT NULL,\n-    PRIMARY KEY (entity_name, content_type)\n-);\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_client_triggers.sql",
            "patch": "@@ -0,0 +1,16 @@\n+SET SEARCH_PATH = client_graph;\n+\n+-- stemmed_domain_ihq_topics\n+CREATE OR REPLACE TRIGGER stemmed_domain_ihq_topics_prevent_update BEFORE UPDATE ON stemmed_domain_ihq_topics\n+    EXECUTE FUNCTION prevent_update();\n+\n+-- stemmed_domain_named_entities\n+CREATE OR REPLACE TRIGGER stemmed_domain_named_entities_prevent_update BEFORE UPDATE ON stemmed_domain_named_entities\n+    EXECUTE FUNCTION prevent_update();\n+\n+-- language_preferences\n+CREATE OR REPLACE TRIGGER language_preferences_pre_ins BEFORE INSERT ON language_preferences\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER language_preferences_pre_upd BEFORE update ON language_preferences\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_common_triggers.sql",
            "patch": "@@ -0,0 +1,139 @@\n+SET SEARCH_PATH = :schema_name;\n+\n+CREATE OR REPLACE FUNCTION pre_insert_audit() RETURNS trigger AS $$\n+BEGIN\n+    NEW.created_at := current_timestamp;\n+    NEW.created_by := current_setting('my.application');\n+    RETURN NEW;\n+END;\n+$$ LANGUAGE plpgsql;\n+\n+CREATE OR REPLACE FUNCTION pre_update_audit() RETURNS trigger AS $$\n+BEGIN\n+    IF NEW.created_at != OLD.created_at OR NEW.created_by != OLD.created_by THEN\n+        RAISE EXCEPTION 'Audit columns created_at and created_by may not be updated';\n+    END IF;\n+    NEW.modified_at := current_timestamp;\n+    NEW.modified_by := current_setting('my.application');\n+    RETURN NEW;\n+END;\n+$$ LANGUAGE plpgsql;\n+\n+CREATE OR REPLACE FUNCTION prevent_update() RETURNS trigger AS $$\n+BEGIN\n+    RAISE EXCEPTION 'Update not permitted - record should be deleted and replaced';\n+END\n+$$ LANGUAGE plpgsql;\n+\n+-- novatiq_iab_categories\n+CREATE OR REPLACE TRIGGER novatiq_iab_categories_pre_ins BEFORE INSERT ON novatiq_iab_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+\n+CREATE OR REPLACE TRIGGER novatiq_iab_categories_pre_upd BEFORE UPDATE ON novatiq_iab_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+-- ihq_topics\n+CREATE OR REPLACE TRIGGER ihq_topics_pre_ins BEFORE INSERT ON ihq_topics\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER ihq_topics_pre_upd BEFORE UPDATE ON ihq_topics\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- ihq_topic_parents\n+CREATE OR REPLACE TRIGGER ihq_topic_parents_pre_ins BEFORE INSERT ON ihq_topic_parents\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER ihq_topic_parents_prevent_upd BEFORE UPDATE ON ihq_topic_parents\n+    FOR EACH ROW EXECUTE FUNCTION prevent_update();\n+\n+\n+-- ihq_topic_novatiq_iab_categories\n+CREATE OR REPLACE TRIGGER ihq_topic_novatiq_iab_categories_pre_ins BEFORE INSERT ON ihq_topic_novatiq_iab_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER ihq_topic_novatiq_iab_categories_pre_upd BEFORE UPDATE ON ihq_topic_novatiq_iab_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- ihq_topic_labels\n+CREATE OR REPLACE TRIGGER ihq_topic_labels_pre_ins BEFORE INSERT ON ihq_topic_labels\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER ihq_topic_labels_pre_upd BEFORE UPDATE ON ihq_topic_labels\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- stemmed_domains\n+CREATE OR REPLACE TRIGGER stemmed_domains_pre_ins BEFORE INSERT ON stemmed_domains\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER stemmed_domains_prevent_upd BEFORE UPDATE ON stemmed_domains\n+    EXECUTE FUNCTION prevent_update();\n+\n+\n+-- stemmed_domain_ihq_topics\n+CREATE OR REPLACE TRIGGER stemmed_domain_ihq_topics_pre_ins BEFORE INSERT ON stemmed_domain_ihq_topics\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+\n+-- named_entities\n+CREATE OR REPLACE TRIGGER named_entities_pre_ins BEFORE INSERT ON named_entities\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER named_entities_pre_upd BEFORE UPDATE ON named_entities\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- named_entity_labels\n+CREATE OR REPLACE TRIGGER named_entity_labels_pre_ins BEFORE INSERT ON named_entity_labels\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER named_entity_labels_pre_upd BEFORE UPDATE ON named_entity_labels\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- wiki_brands\n+CREATE OR REPLACE TRIGGER wiki_brands_pre_ins BEFORE INSERT ON wiki_brands\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER wiki_brands_pre_upd BEFORE UPDATE ON wiki_brands\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- wiki_brand_labels\n+CREATE OR REPLACE TRIGGER wiki_brand_labels_pre_ins BEFORE INSERT ON wiki_brand_labels\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER wiki_brand_labels_pre_upd BEFORE UPDATE ON wiki_brand_labels\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- stemmed_domain_named_entities\n+CREATE OR REPLACE TRIGGER stemmed_domain_named_entities_pre_ins BEFORE INSERT ON stemmed_domain_named_entities\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+\n+-- content_categories\n+CREATE OR REPLACE TRIGGER content_categories_pre_ins BEFORE INSERT ON content_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER content_categories_pre_upd BEFORE UPDATE ON content_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- ihq_topic_content_categories\n+CREATE OR REPLACE TRIGGER ihq_topic_content_categories_pre_ins BEFORE INSERT ON ihq_topic_content_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER ihq_topic_content_categories_pre_upd BEFORE UPDATE ON ihq_topic_content_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+\n+-- named_entity_content_categories\n+CREATE OR REPLACE TRIGGER named_entity_content_categories_pre_ins BEFORE INSERT ON named_entity_content_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+CREATE OR REPLACE TRIGGER named_entity_content_categories_pre_upd BEFORE UPDATE ON named_entity_content_categories\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_ihq_schema.sql",
            "patch": "@@ -1,49 +1,72 @@\n CREATE SCHEMA IF NOT EXISTS ihq_graph;\n+SET my.application = \"Initial Load\";\n \n SET SEARCH_PATH = ihq_graph;\n \n -- Distinguish these from the regular IAB categories, as they differ and we may want to use both later\n CREATE TABLE IF NOT EXISTS novatiq_iab_categories (\n-   novatiq_id   VARCHAR(10) PRIMARY KEY,\n-   name         VARCHAR(255) NOT NULL,\n-   extension    VARCHAR(3),\n-   parent_id    VARCHAR(10),\n-   FOREIGN KEY (parent_id) REFERENCES novatiq_iab_categories(novatiq_id)\n+    novatiq_id   VARCHAR(10)  PRIMARY KEY,\n+    name         VARCHAR(255) NOT NULL,\n+    extension    VARCHAR(3),\n+    parent_id    VARCHAR(10),\n+    created_by   VARCHAR(100) NOT NULL,\n+    created_at   TIMESTAMP    NOT NULL,\n+    modified_by  VARCHAR(100),\n+    modified_at  TIMESTAMP,\n+    FOREIGN KEY (parent_id) REFERENCES novatiq_iab_categories(novatiq_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topics (\n-    topic_id       VARCHAR(50) PRIMARY KEY,\n-    default_label  VARCHAR(255) NOT NULL,\n-    definition     VARCHAR,\n-    is_virtual     BOOLEAN NOT NULL\n+    topic_id        VARCHAR(50)  PRIMARY KEY,\n+    default_label   VARCHAR(255) NOT NULL,\n+    definition      VARCHAR,\n+    is_virtual      BOOLEAN      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP\n );\n \n+-- No modified columns; many-to-many relationship means changing parent or child is a new record\n CREATE TABLE IF NOT EXISTS ihq_topic_parents(\n-    topic_id    VARCHAR(50) NOT NULL,\n-    parent_id   VARCHAR(50) NOT NULL,\n+    topic_id    VARCHAR(50)  NOT NULL,\n+    parent_id   VARCHAR(50)  NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n     PRIMARY KEY (topic_id, parent_id),\n     FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id),\n     FOREIGN KEY (parent_id) REFERENCES ihq_topics(topic_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topic_novatiq_iab_categories(\n-    topic_id    VARCHAR(50) NOT NULL,\n-    novatiq_id  VARCHAR(10) NOT NULL,\n+    topic_id    VARCHAR(50)  NOT NULL,\n+    novatiq_id  VARCHAR(10)  NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (topic_id, novatiq_id),\n     FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id),\n     FOREIGN KEY (novatiq_id) REFERENCES novatiq_iab_categories(novatiq_id)\n );\n \n CREATE TABLE IF NOT EXISTS ihq_topic_labels (\n-    topic_id VARCHAR(50) NOT NULL,\n-    lang     VARCHAR(2) NOT NULL,\n-    label    VARCHAR(255) NOT NULL,\n+    topic_id    VARCHAR(50)  NOT NULL,\n+    lang        VARCHAR(2)   NOT NULL,\n+    label       VARCHAR(255) NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (topic_id, lang),\n     FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id)\n );\n \n+-- No modified columns; editing the domain is a new record\n CREATE TABLE IF NOT EXISTS stemmed_domains (\n-    stemmed_domain VARCHAR(100) PRIMARY KEY\n+    stemmed_domain  VARCHAR(100) PRIMARY KEY,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL\n );\n \n CREATE TABLE IF NOT EXISTS stemmed_domain_metadata (\n@@ -53,12 +76,21 @@ CREATE TABLE IF NOT EXISTS stemmed_domain_metadata (\n     body            VARCHAR,\n     source          VARCHAR(100),\n     source_version  VARCHAR(100),\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     FOREIGN KEY (stemmed_domain) REFERENCES stemmed_domains(stemmed_domain)\n );\n \n CREATE TABLE IF NOT EXISTS stemmed_domain_ihq_topics (\n-    stemmed_domain VARCHAR(100) NOT NULL,\n-    topic_id       VARCHAR(50) NOT NULL,\n+    stemmed_domain  VARCHAR(100) NOT NULL,\n+    topic_id        VARCHAR(50)  NOT NULL,\n+    score           NUMERIC      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     PRIMARY KEY (stemmed_domain, topic_id),\n     FOREIGN KEY (stemmed_domain) REFERENCES stemmed_domains(stemmed_domain),\n     FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id)\n@@ -69,6 +101,10 @@ CREATE TABLE IF NOT EXISTS stemmed_domain_ihq_topic_metadata (\n     topic_id        VARCHAR(100) NOT NULL,\n     source          VARCHAR(100),\n     source_version  VARCHAR(100),\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     PRIMARY KEY (stemmed_domain, topic_id),\n     FOREIGN KEY (stemmed_domain) REFERENCES stemmed_domains(stemmed_domain),\n     FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id)\n@@ -77,36 +113,57 @@ CREATE TABLE IF NOT EXISTS stemmed_domain_ihq_topic_metadata (\n CREATE INDEX IF NOT EXISTS stemmed_domain_ihq_topics_topic_id_ix ON stemmed_domain_ihq_topics(topic_id);\n \n CREATE TABLE IF NOT EXISTS named_entities (\n-    entity_name   VARCHAR(255) PRIMARY KEY,\n-    default_label VARCHAR(255) NOT NULL\n+    entity_name     VARCHAR(255) PRIMARY KEY,\n+    default_label   VARCHAR(255) NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP\n );\n \n CREATE TABLE IF NOT EXISTS named_entity_labels (\n     entity_name VARCHAR(255) NOT NULL,\n-    lang        VARCHAR(2) NOT NULL,\n+    lang        VARCHAR(2)   NOT NULL,\n     label       VARCHAR(255) NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP    NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (entity_name, lang),\n     FOREIGN KEY (entity_name) REFERENCES named_entities(entity_name)\n );\n \n CREATE TABLE IF NOT EXISTS wiki_brands (\n-    wiki_id         INTEGER PRIMARY KEY,\n+    wiki_id         INTEGER      PRIMARY KEY,\n     default_label   VARCHAR(255) NOT NULL,\n     entity_name     VARCHAR(255) NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     FOREIGN KEY (entity_name) REFERENCES named_entities(entity_name)\n );\n \n CREATE TABLE IF NOT EXISTS wiki_brand_labels (\n-    wiki_id INTEGER,\n-    lang    VARCHAR(2),\n-    label   VARCHAR(255) NOT NULL,\n+    wiki_id     INTEGER      NOT NULL,\n+    lang        VARCHAR(2)   NOT NULL,\n+    label       VARCHAR(255) NOT NULL,\n+    created_by  VARCHAR(100) NOT NULL,\n+    created_at  TIMESTAMP NOT NULL,\n+    modified_by VARCHAR(100),\n+    modified_at TIMESTAMP,\n     PRIMARY KEY (wiki_id, lang),\n     FOREIGN KEY (wiki_id) REFERENCES wiki_brands(wiki_id)\n );\n \n CREATE TABLE IF NOT EXISTS stemmed_domain_named_entities (\n-    stemmed_domain VARCHAR(100),\n-    entity_name    VARCHAR(255),\n+    stemmed_domain  VARCHAR(100),\n+    entity_name     VARCHAR(255),\n+    score           NUMERIC      NOT NULL,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     PRIMARY KEY (stemmed_domain, entity_name),\n     FOREIGN KEY (stemmed_domain) REFERENCES stemmed_domains(stemmed_domain),\n     FOREIGN KEY (entity_name) REFERENCES named_entities(entity_name)\n@@ -119,7 +176,50 @@ CREATE TABLE IF NOT EXISTS stemmed_domain_named_entity_metadata (\n     entity_name     VARCHAR(255) NOT NULL,\n     source          VARCHAR(100),\n     source_version  VARCHAR(100),\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n     PRIMARY KEY (stemmed_domain, entity_name),\n     FOREIGN KEY (stemmed_domain) REFERENCES stemmed_domains(stemmed_domain),\n     FOREIGN KEY (entity_name) REFERENCES named_entities(entity_name)\n+);\n+\n+CREATE TABLE IF NOT EXISTS content_categories (\n+    content_type        VARCHAR(50)  PRIMARY KEY,\n+    description         VARCHAR,\n+    block_level         INTEGER,\n+    total_block_level   INTEGER,\n+    created_by          VARCHAR(100) NOT NULL,\n+    created_at          TIMESTAMP    NOT NULL,\n+    modified_by         VARCHAR(100),\n+    modified_at         TIMESTAMP\n+);\n+\n+CREATE TABLE IF NOT EXISTS ihq_topic_content_categories (\n+    topic_id        VARCHAR(50)  NOT NULL,\n+    content_type    VARCHAR(50)  NOT NULL,\n+    content_level   INTEGER      NOT NULL,\n+    notes           VARCHAR,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n+    PRIMARY KEY (topic_id, content_type),\n+    FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id),\n+    FOREIGN KEY (content_type) REFERENCES content_categories(content_type)\n+);\n+\n+CREATE TABLE IF NOT EXISTS named_entity_content_categories (\n+    entity_name     VARCHAR(255) NOT NULL,\n+    content_type    VARCHAR(50)  NOT NULL,\n+    content_level   INTEGER      NOT NULL,\n+    notes           VARCHAR,\n+    created_by      VARCHAR(100) NOT NULL,\n+    created_at      TIMESTAMP    NOT NULL,\n+    modified_by     VARCHAR(100),\n+    modified_at     TIMESTAMP,\n+    PRIMARY KEY (entity_name, content_type),\n+    FOREIGN KEY (entity_name) REFERENCES named_entities(entity_name),\n+    FOREIGN KEY (content_type) REFERENCES content_categories(content_type)\n );\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_ihq_schema_blocklist.sql",
            "patch": "@@ -1,28 +0,0 @@\n-SET SEARCH_PATH = ihq_graph;\n-\n-CREATE TABLE IF NOT EXISTS content_categories (\n-    content_type        VARCHAR(50) PRIMARY KEY,\n-    description         VARCHAR,\n-    block_level         INTEGER,\n-    total_block_level   INTEGER\n-);\n-\n-CREATE TABLE IF NOT EXISTS ihq_topic_content_categories (\n-    topic_id        VARCHAR(50) NOT NULL,\n-    content_type    VARCHAR(50) NOT NULL,\n-    content_level   INTEGER NOT NULL,\n-    notes           VARCHAR,\n-    PRIMARY KEY (topic_id, content_type),\n-    FOREIGN KEY (topic_id) REFERENCES ihq_topics(topic_id),\n-    FOREIGN KEY (content_type) REFERENCES content_categories(content_type)\n-);\n-\n-CREATE TABLE IF NOT EXISTS named_entity_content_categories (\n-    entity_name     VARCHAR(255) NOT NULL,\n-    content_type    VARCHAR(50) NOT NULL,\n-    content_level   INTEGER NOT NULL,\n-    notes           VARCHAR,\n-    PRIMARY KEY (entity_name, content_type),\n-    FOREIGN KEY (entity_name) REFERENCES named_entities(entity_name),\n-    FOREIGN KEY (content_type) REFERENCES content_categories(content_type)\n-);\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_ihq_triggers.sql",
            "patch": "@@ -0,0 +1,33 @@\n+SET SEARCH_PATH = 'ihq_graph';\n+\n+-- stemmed_domain_ihq_topics\n+CREATE OR REPLACE TRIGGER stemmed_domain_ihq_topics_pre_upd BEFORE UPDATE ON stemmed_domain_ihq_topics\n+    EXECUTE FUNCTION pre_update_audit();\n+\n+-- stemmed_domain_named_entities\n+CREATE OR REPLACE TRIGGER stemmed_domain_named_entities_pre_upd BEFORE UPDATE ON stemmed_domain_named_entities\n+    EXECUTE FUNCTION pre_update_audit();\n+\n+-- stemmed_domain_metadata\n+CREATE OR REPLACE TRIGGER stemmed_domain_metadata_pre_ins BEFORE INSERT ON stemmed_domain_metadata\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+\n+CREATE OR REPLACE TRIGGER stemmed_domain_metadata_pre_upd BEFORE UPDATE ON stemmed_domain_metadata\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+-- stemmed_domain_ihq_topic_metadata\n+CREATE OR REPLACE TRIGGER stemmed_domain_ihq_topic_metadata_pre_ins BEFORE INSERT ON stemmed_domain_ihq_topic_metadata\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+\n+CREATE OR REPLACE TRIGGER stemmed_domain_ihq_topic_metadata_pre_upd BEFORE UPDATE ON stemmed_domain_ihq_topic_metadata\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();\n+\n+-- stemmed_domain_named_entity_metadata\n+CREATE OR REPLACE TRIGGER stemmed_domain_named_entity_metadata_pre_ins BEFORE INSERT ON stemmed_domain_named_entity_metadata\n+    FOR EACH ROW EXECUTE FUNCTION pre_insert_audit();\n+\n+\n+CREATE OR REPLACE TRIGGER stemmed_domain_named_entity_metadata_pre_upd BEFORE UPDATE ON stemmed_domain_named_entity_metadata\n+    FOR EACH ROW EXECUTE FUNCTION pre_update_audit();"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/create_views.sql",
            "patch": "@@ -112,4 +112,55 @@ CREATE OR REPLACE VIEW stemmed_domain_named_entities AS\n \n CREATE OR REPLACE VIEW  language_preferences AS\n     SELECT lang, preference_order\n-    FROM client_graph.language_preferences;\n\\ No newline at end of file\n+    FROM client_graph.language_preferences;\n+\n+CREATE OR REPLACE VIEW content_categories AS\n+    SELECT COALESCE(c.content_type, i.content_type) AS content_type,\n+           COALESCE(c.description, i.description) AS description,\n+           COALESCE(c.block_level, i.block_level) AS block_level,\n+           LEAST(c.total_block_level, i.total_block_level) AS total_block_level\n+    FROM ihq_graph.content_categories i\n+    FULL OUTER JOIN client_graph.content_categories c ON\n+        c.content_type = i.content_type;\n+\n+CREATE OR REPLACE VIEW ihq_topic_content_categories AS\n+    SELECT  COALESCE(c.topic_id, i.topic_id) AS topic_id,\n+            COALESCE(c.content_type, i.content_type) AS content_type,\n+            CASE\n+                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level AND\n+                     c.content_level IS NOT NULL\n+                THEN GREATEST(cc.total_block_level, c.content_level)\n+                ELSE COALESCE(c.content_level, i.content_level)\n+            END AS content_level,\n+            COALESCE(c.notes, i.notes) AS notes,\n+            CASE\n+                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level THEN False\n+                ELSE COALESCE(c.exclude, False)\n+            END AS exclude\n+    FROM ihq_graph.ihq_topic_content_categories i\n+    JOIN ihq_graph.content_categories cc ON\n+        i.content_type = cc.content_type\n+    FULL OUTER JOIN client_graph.ihq_topic_content_categories c ON\n+        c.topic_id = i.topic_id AND\n+        c.content_type = i.content_type;\n+\n+CREATE OR REPLACE VIEW named_entity_content_categories AS\n+    SELECT  COALESCE(c.entity_name, i.entity_name) AS entity_name,\n+            COALESCE(c.content_type, i.content_type) AS content_type,\n+            CASE\n+                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level AND\n+                     c.content_level IS NOT NULL\n+                THEN GREATEST(cc.total_block_level, c.content_level)\n+                ELSE COALESCE(c.content_level, i.content_level)\n+            END AS content_level,\n+            COALESCE(c.notes, i.notes) AS notes,\n+            CASE\n+                WHEN COALESCE(i.content_level, -1) >= cc.total_block_level THEN False\n+                ELSE COALESCE(c.exclude, False)\n+            END AS exclude\n+    FROM ihq_graph.named_entity_content_categories i\n+    JOIN content_categories cc ON\n+        i.content_type = cc.content_type\n+    FULL OUTER JOIN client_graph.named_entity_content_categories c ON\n+        c.entity_name = i.entity_name AND\n+        c.content_type = i.content_type;\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/ddl_run_order.txt",
            "patch": "@@ -1,9 +1,9 @@\n ./resources/postgres/DDL/create_ihq_schema.sql\n ./resources/postgres/DDL/create_client_schema.sql\n ./resources/postgres/DDL/create_views.sql\n-./resources/postgres/DDL/create_ihq_schema_blocklist.sql\n-./resources/postgres/DDL/create_client_schema_blocklist.sql\n-./resources/postgres/DDL/create_blocklist_views.sql\n+./resources/postgres/DDL/create_common_triggers.sql\n+./resources/postgres/DDL/create_ihq_triggers.sql\n+./resources/postgres/DDL/create_client_triggers.sql\n ./resources/postgres/query/labelled_ihq_topics.sql\n ./resources/postgres/query/labelled_named_entities.sql\n ./resources/postgres/query/labelled_wiki_brands.sql"
        },
        {
            "filename": "ihq-graph/resources/postgres/DDL/run_ddl.sh",
            "patch": "@@ -0,0 +1,19 @@\n+function check_var() {\n+  eval \"if [[ -z \\${$1} ]]; then echo environment variable $1 must be defined. && exit 1; fi; \"\n+}\n+\n+dirname $0\n+\n+for var in PGUSER PGDATABASE PGPASSWORD; do\n+  check_var $var\n+done\n+\n+while read -r script; do\n+  echo \"$script\";\n+  if [[ \"$script\" == *\"common\"* ]]; then\n+    psql -h localhost -p 5455 -v schema_name=ihq_graph < \"$script\";\n+    psql -h localhost -p 5455 -v schema_name=client_graph < \"$script\";\n+  else\n+    psql -h localhost -p 5455 < \"$script\";\n+  fi;\n+done < $(dirname $0)/ddl_run_order.txt\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/README.md",
            "patch": "@@ -14,7 +14,7 @@ docker run -p 5455:5432 \\\n       -e POSTGRES_USER=postgresUser \\\n       -e POSTGRES_PASSWORD=postgresPW \\\n       -e POSTGRES_DB=postgresDB \\\n-      -v /Users/catherinedegraca/workspace/mle/ihq-graph/postgres/data:/data/load\n+      -v /Users/catherinedegraca/workspace/mle/ihq-graph/postgres/data:/data/load \\\n       -d postgres:17\n ```\n ## Running on staging"
        },
        {
            "filename": "ihq-graph/resources/postgres/load/load_blocklist.sql",
            "patch": "@@ -1,4 +1,5 @@\n SET search_path = ihq_graph;\n+SET my.application = 'KB_Initialisation';\n \n INSERT INTO content_categories (content_type, description, block_level, total_block_level)\n VALUES  ('Blocked', 'Any potentially sensitive topics', 3, 5),"
        },
        {
            "filename": "ihq-graph/resources/postgres/load/load_ihq_topics.sql",
            "patch": "@@ -1,4 +1,5 @@\n SET SEARCH_PATH = ihq_graph;\n+SET my.application = 'KB_Initialisation';\n \n CREATE TEMPORARY TABLE tmp_topic_parents (\n     topic_id        VARCHAR NOT NULL,\n@@ -46,8 +47,8 @@ INSERT INTO stemmed_domains (stemmed_domain)\n     FROM tmp_domain_topics\n     ON CONFLICT DO NOTHING;\n \n-INSERT INTO stemmed_domain_ihq_topics (stemmed_domain, topic_id)\n-    SELECT stemmed_domain, topic_id\n+INSERT INTO stemmed_domain_ihq_topics (stemmed_domain, topic_id, score)\n+    SELECT stemmed_domain, topic_id, 1\n     FROM tmp_domain_topics t;\n \n DROP TABLE tmp_topic_parents;"
        },
        {
            "filename": "ihq-graph/resources/postgres/load/load_wiki_brands.sql",
            "patch": "@@ -1,4 +1,5 @@\n SET SEARCH_PATH = ihq_graph;\n+SET my.application = 'KB_Initialisation';\n \n CREATE TEMPORARY TABLE tmp_wiki_brands (\n     wiki_id         INTEGER NOT NULL,\n@@ -39,8 +40,8 @@ INSERT INTO stemmed_domains\n     WHERE LENGTH(stemmed_domain) < 100\n     ON CONFLICT DO NOTHING;\n \n-INSERT INTO stemmed_domain_named_entities (stemmed_domain, entity_name)\n-    SELECT DISTINCT stemmed_domain, entity_name\n+INSERT INTO stemmed_domain_named_entities (stemmed_domain, entity_name, score)\n+    SELECT DISTINCT stemmed_domain, entity_name, 1\n     FROM tmp_domain_brands t\n     JOIN tmp_wiki_brands w\n       ON w.wiki_id = t.wiki_id"
        },
        {
            "filename": "ihq-graph/resources/postgres/test/pg_test_helper.py",
            "patch": "@@ -19,11 +19,13 @@ def __init__(self, tablename, columns, data):\n         self.data = data\n \n \n-def connect(postgresql):\n+def connect(postgresql, application_name):\n     dsn = postgresql.dsn()\n     dsn[\"dbname\"] = dsn[\"database\"]\n     del dsn[\"database\"]\n-    return psycopg.connect(**dsn)\n+    conn = psycopg.connect(**dsn)\n+    conn.execute(f\"SET my.application = '{application_name}'\")\n+    return conn\n \n \n def insert_rows(conn, table, columns, data):\n@@ -33,24 +35,33 @@ def insert_rows(conn, table, columns, data):\n             cur.executemany(insert_string, data)\n \n \n-def handler(postgresql, datasets):\n-    conn = connect(postgresql)\n-    with conn.cursor() as cursor:\n-        for ddl_file in open(\"./resources/postgres/test/ddl_run_order.txt\").readlines():\n-            cursor.execute(open(ddl_file.strip(\"\\n\")).read())\n-    for dataset in datasets:\n-        insert_rows(conn, dataset.tablename, dataset.columns, dataset.data)\n-    conn.commit()\n-    conn.close()\n+def handler(postgresql, application_name, datasets, ddl_run_file):\n+    if ddl_run_file is not None:\n+        schemas = [\"ihq_graph\", \"client_graph\"]\n+        conn = connect(postgresql, application_name)\n+        with conn.cursor() as cursor:\n+            for ddl_file in open(ddl_run_file).readlines():\n+                apply_schemas = schemas if \"common\" in ddl_file else [\"\"]\n+                for schema_name in apply_schemas:\n+                    cursor.execute(open(ddl_file.strip(\"\\n\")).read().replace(\":schema_name\", schema_name))\n+        for dataset in datasets:\n+            insert_rows(conn, dataset.tablename, dataset.columns, dataset.data)\n+        conn.commit()\n+        conn.close()\n \n \n class PgTestHelper:\n \n     datasets = []\n+    ddl_run_file = \"resources/postgres/DDL/ddl_run_order.txt\"\n+    application_name = \"pg_test_helper\"\n \n     @fixture(scope=\"session\")\n     def pg_conn(self):\n-        h = partial(handler, datasets=self.datasets)\n+        h = partial(handler,\n+                    application_name=self.application_name,\n+                    datasets=self.datasets,\n+                    ddl_run_file=self.ddl_run_file)\n         pg_conn = testing.postgresql.PostgresqlFactory(cache_initialized_db=True,\n                                                        on_initialized=h)()\n         yield pg_conn"
        },
        {
            "filename": "ihq-graph/resources/postgres/test/test_audits.py",
            "patch": "@@ -0,0 +1,190 @@\n+import time\n+\n+import pytest\n+from psycopg.errors import RaiseException\n+from pytest import ExceptionInfo\n+\n+from datetime import datetime\n+\n+from resources.postgres.test.pg_test_helper import connect, insert_rows, PgTestDataset, PgTestHelper\n+\n+\n+class TestTableAudits(PgTestHelper):\n+    application_name = \"audit_test\"\n+\n+    insert_only_tables = [\"ihq_graph.stemmed_domains\",\n+                          \"client_graph.stemmed_domains\",\n+                          \"ihq_graph.ihq_topic_parents\",\n+                          \"client_graph.ihq_topic_parents\",\n+                          \"client_graph.stemmed_domain_ihq_topics\",\n+                          \"client_graph.stemmed_domain_named_entities\"]\n+\n+    def test_table_audit_columns(self, pg_conn):\n+        expected_create_columns = [\n+            (\"created_at\", \"timestamp without time zone\", \"NO\"),\n+            (\"created_by\", \"character varying\", \"NO\")\n+        ]\n+        expected_modify_columns = [\n+            (\"modified_at\", \"timestamp without time zone\", \"YES\"),\n+            (\"modified_by\", \"character varying\", \"YES\")\n+            ]\n+        conn = connect(pg_conn, self.application_name)\n+        with conn.cursor() as cursor:\n+            for schema in [\"ihq_graph\", \"client_graph\"]:\n+                tablenames = [r[0] for r in cursor.execute(\"SELECT table_name FROM information_schema.tables \"\n+                                                           f\"WHERE table_schema = '{schema}'\").fetchall()]\n+                for tablename in tablenames:\n+                    columns = cursor.execute(\"SELECT column_name, data_type, is_nullable \" \n+                                             \"FROM information_schema.columns \" \n+                                             f\"WHERE table_schema = '{schema}' AND table_name = '{tablename}' \"\n+                                             \"ORDER BY column_name\").fetchall()\n+                    assert all([expected in columns for expected in expected_create_columns])\n+                    if f\"{schema}.{tablename}\" not in self.insert_only_tables:\n+                        assert all([expected in columns for expected in expected_modify_columns])\n+\n+    def test_insert_update_triggers(self, pg_conn):\n+        conn = connect(pg_conn, self.application_name)\n+        with conn.cursor() as cursor:\n+            for schema in [\"ihq_graph\", \"client_graph\"]:\n+                tablenames = [r[0] for r in cursor.execute(\"SELECT table_name FROM information_schema.tables \"\n+                                                           f\"WHERE table_schema = '{schema}'\").fetchall()]\n+                for tablename in tablenames:\n+                    triggers = cursor.execute(\"SELECT event_manipulation, action_timing, action_statement \" \n+                                              \"FROM information_schema.triggers \"\n+                                              f\"WHERE event_object_schema = '{schema}' and event_object_table = '{tablename}'\"\n+                                              ).fetchall()\n+                    assert (\"INSERT\", \"BEFORE\", f\"EXECUTE FUNCTION {schema}.pre_insert_audit()\") in triggers\n+                    if f\"{schema}.{tablename}\" in self.insert_only_tables:\n+                        assert (\"UPDATE\", \"BEFORE\", f\"EXECUTE FUNCTION {schema}.prevent_update()\") in triggers\n+                    else:\n+                        assert (\"UPDATE\", \"BEFORE\", f\"EXECUTE FUNCTION {schema}.pre_update_audit()\") in triggers\n+\n+    \"\"\"\n+    As long as all of the triggers use the same function (verified above), we only need to test operations on one table\n+    \"\"\"\n+    def test_insert_trigger_function_sets_created(self, pg_conn):\n+        conn = connect(pg_conn, self.application_name)\n+        insert_entity_name = \"ins entity\"\n+        insert_entity_label = \"entity label\"\n+        with conn.cursor() as cursor:\n+            create_time = datetime.now()\n+            insert_rows(conn, \"ihq_graph.named_entities\",\n+                        [\"entity_name\", \"default_label\"],\n+                        [(insert_entity_name, insert_entity_label)])\n+            (entity_name,\n+             default_label,\n+             created_by,\n+             created_at,\n+             modified_by,\n+             modified_at\n+             ) = cursor.execute(\"SELECT entity_name, default_label, created_by, created_at, modified_by, modified_at \"\n+                                f\"FROM ihq_graph.named_entities WHERE entity_name = '{insert_entity_name}'\"\n+                                ).fetchall()[0]\n+        conn.rollback()\n+        conn.close()\n+        assert (entity_name, default_label, created_by, modified_by, modified_at) == (\n+            insert_entity_name, insert_entity_label, self.application_name, None, None\n+        )\n+        # check timestamp is within 2 seconds of expected - not sure how precisely we can guarantee matching\n+        assert created_at.timestamp() == pytest.approx(create_time.timestamp(), abs=10)\n+\n+    \"\"\"\n+    Verify the pre-update trigger function\n+    \"\"\"\n+    def test_update_trigger_function_sets_modified(self, pg_conn):\n+        conn = connect(pg_conn, self.application_name)\n+        update_entity_name = \"update entity\"\n+        initial_label = \"label\"\n+        update_label = \"new label\"\n+        with conn.cursor() as cursor:\n+            create_time = datetime.now()\n+            insert_rows(conn, \"ihq_graph.named_entities\",\n+                        [\"entity_name\", \"default_label\"],\n+                        [(update_entity_name, initial_label)])\n+            (entity_name,\n+             default_label,\n+             created_by,\n+             created_at,\n+             modified_by,\n+             modified_at\n+             ) = cursor.execute(\"SELECT entity_name, default_label, created_by, created_at, modified_by, modified_at \"\n+                                f\"FROM ihq_graph.named_entities WHERE entity_name = '{update_entity_name}'\"\n+                                ).fetchall()[0]\n+\n+        assert (entity_name, default_label, created_by, modified_by, modified_at) == (\n+            update_entity_name, initial_label, self.application_name, None, None\n+        )\n+\n+        # We have to commit, end the transaction, and start a new one, if we want the modified time to differ from\n+        # created - current_timestamp is always the timestamp at  which the transaction started and remains the same\n+        # throughout that transaction\n+        conn.commit()\n+        conn.close\n+        time.sleep(10)\n+        with conn.cursor() as cursor:\n+            modified_time = datetime.now()\n+            cursor.execute(\"UPDATE ihq_graph.named_entities \"\n+                           f\"SET default_label = '{update_label}' WHERE entity_name = '{update_entity_name}'\")\n+            (entity_name,\n+             default_label,\n+             created_by,\n+             created_at,\n+             modified_by,\n+             modified_at\n+             ) = cursor.execute(\"SELECT entity_name, default_label, created_by, created_at, modified_by, modified_at \"\n+                                f\"FROM ihq_graph.named_entities WHERE entity_name = '{update_entity_name}'\"\n+                                ).fetchall()[0]\n+            assert (entity_name, default_label, created_by, modified_by) == (\n+                update_entity_name, update_label, self.application_name, self.application_name\n+            )\n+        # check timestamps are within 2 seconds of expected - not sure how precisely we can guarantee matching\n+        assert created_at.timestamp() == pytest.approx(create_time.timestamp(), abs=2)\n+        assert modified_at.timestamp() == pytest.approx(modified_time.timestamp(), abs=2)\n+        conn.rollback()\n+        conn.close()\n+\n+    def test_cannot_update_created_by(self, pg_conn):\n+        conn = connect(pg_conn, self.application_name)\n+        error_entity_name = \"error entity\"\n+        entity_label = \"label\"\n+        with conn.cursor() as cursor:\n+            insert_rows(conn, \"ihq_graph.named_entities\",\n+                        [\"entity_name\", \"default_label\"],\n+                        [(error_entity_name, entity_label)])\n+\n+            with pytest.raises(RaiseException) as err:\n+                cursor.execute(\"UPDATE ihq_graph.named_entities \"\n+                               f\"SET created_by = 'not_allowed' WHERE entity_name = '{error_entity_name}'\")\n+        assert str(err.value).startswith(\"Audit columns created_at and created_by may not be updated\")\n+        conn.rollback()\n+        conn.close()\n+\n+    def test_cannot_update_created_at(self, pg_conn):\n+        conn = connect(pg_conn, self.application_name)\n+        error_entity_name = \"error entity\"\n+        entity_label = \"label\"\n+        with conn.cursor() as cursor:\n+            insert_rows(conn, \"ihq_graph.named_entities\",\n+                        [\"entity_name\", \"default_label\"],\n+                        [(error_entity_name, entity_label)])\n+\n+            with pytest.raises(RaiseException) as err:\n+                cursor.execute(\"UPDATE ihq_graph.named_entities \"\n+                               f\"SET created_at = '{datetime.now()}' WHERE entity_name = '{error_entity_name}'\")\n+        assert str(err.value).startswith(\"Audit columns created_at and created_by may not be updated\")\n+        conn.rollback()\n+        conn.close()\n+\n+    \"\"\"\n+    Verify the prevent-update trigger function\n+    \"\"\"\n+    def test_prevent_update_trigger_function(self, pg_conn):\n+        conn = connect(pg_conn, self.application_name)\n+        with conn.cursor() as cursor:\n+            insert_rows(conn, \"ihq_graph.stemmed_domains\", [\"stemmed_domain\"], [(\"domain.com\",)])\n+            with pytest.raises(RaiseException) as err:\n+                cursor.execute(\"UPDATE ihq_graph.stemmed_domains set stemmed_domain = 'new.domain' \"\n+                               \"WHERE stemmed_domain = 'domain.com'\")\n+        assert str(err.value).startswith(\"Update not permitted - record should be deleted and replaced\")\n+        conn.rollback()\n+        conn.close()\n\\ No newline at end of file"
        },
        {
            "filename": "ihq-graph/resources/postgres/test/test_mapping_views.py",
            "patch": "@@ -24,8 +24,8 @@\n ihq_topics = [(topic_id, topic_default_label, False)]\n wiki_brands = [(wiki_id, wiki_default_label, entity_name)]\n stemmed_domains = [(stemmed_domain,)]\n-stemmed_domain_named_entities = [(stemmed_domain, entity_name)]\n-stemmed_domain_ihq_topics = [(stemmed_domain, topic_id)]\n+stemmed_domain_named_entities = [(stemmed_domain, entity_name, 0.1)]\n+stemmed_domain_ihq_topics = [(stemmed_domain, topic_id, 1.0)]\n \n \n shared_datasets = [\n@@ -45,9 +45,10 @@ class ViewTestHelper(PgTestHelper):\n     table_name: str\n     columns: list\n     view_data_query: str\n+    application_name: str\n \n     def _run_test(self, connection, new_data, expected_data):\n-        conn = connect(connection)\n+        conn = connect(connection, self.application_name)\n         insert_rows(conn, self.table_name, self.columns, new_data)\n         with conn.cursor() as cursor:\n             view_data = cursor.execute(self.view_data_query).fetchall()\n@@ -139,6 +140,7 @@ def test_content_types_below_above_threshold(self, pg_conn):\n \n \n class TestLabelledNamedEntities(LabelTestHelper):\n+    application_name = \"TestLabelledNamedEntities\"\n     \"\"\"\n     Define the specific data and queries to execute LabelTestHelper for labelled_named_entities\n     \"\"\"\n@@ -160,6 +162,7 @@ class TestLabelledNamedEntities(LabelTestHelper):\n \n \n class TestLabelledIhqTopics(LabelTestHelper):\n+    application_name = \"TestLabelledIhqTopics\"\n     \"\"\"\n     Defines the specific data and queries to execute LabelTestHelper for labelled_ihq_topics\n     \"\"\"\n@@ -182,6 +185,7 @@ class TestLabelledIhqTopics(LabelTestHelper):\n \n \n class TestLabelledWikiBrands(LabelTestHelper):\n+    application_name = \"TestLabelledWikiBrands\"\n     \"\"\"\n     Defines the specific data and queries to execute LabelTestHelper for labelled_wiki_brands\n     \"\"\"\n@@ -205,13 +209,15 @@ class TestLabelledWikiBrands(LabelTestHelper):\n \n \n class TestWebNamedEntityMapping(MappingTestHelper):\n+    application_name = \"TestWebNamedEntityMapping\"\n     \"\"\"\n     Defines the specific data and queries to execute MappingTestHelper for web_named_entity_mapping\n     \"\"\"\n     datasets = shared_datasets + [\n         PgTestDataset(\"ihq_graph.named_entities\", [\"entity_name\", \"default_label\"],\n                       named_entities ),\n-        PgTestDataset(\"ihq_graph.stemmed_domain_named_entities\", [\"stemmed_domain\", \"entity_name\"],\n+        PgTestDataset(\"ihq_graph.stemmed_domain_named_entities\",\n+                      [\"stemmed_domain\", \"entity_name\", \"score\"],\n                       stemmed_domain_named_entities)\n     ]\n \n@@ -228,13 +234,15 @@ class TestWebNamedEntityMapping(MappingTestHelper):\n \n \n class TestWebToppingMapping(MappingTestHelper):\n+    application_name = \"TestWebToppingMapping\"\n     \"\"\"\n     Defines the specific data and queries to execute MappingTestHelper for web_topic_mapping\n     \"\"\"\n     datasets = shared_datasets + [\n         PgTestDataset(\"ihq_graph.ihq_topics\", [\"topic_id\", \"default_label\", \"is_virtual\"],\n                       ihq_topics),\n-        PgTestDataset(\"ihq_graph.stemmed_domain_ihq_topics\", [\"stemmed_domain\", \"topic_id\"],\n+        PgTestDataset(\"ihq_graph.stemmed_domain_ihq_topics\",\n+                      [\"stemmed_domain\", \"topic_id\", \"score\"],\n                       stemmed_domain_ihq_topics)\n     ]\n \n@@ -251,13 +259,15 @@ class TestWebToppingMapping(MappingTestHelper):\n \n \n class TestWebWikiMapping(MappingTestHelper):\n+    application_name = \"TestWebWikiMapping\"\n     \"\"\"\n     Defines the specific data and queries to execute MappingTestHelper for web_wiki_mapping\n     \"\"\"\n     datasets = shared_datasets + [\n         PgTestDataset(\"ihq_graph.named_entities\", [\"entity_name\", \"default_label\"],\n                       named_entities),\n-        PgTestDataset(\"ihq_graph.stemmed_domain_named_entities\", [\"stemmed_domain\", \"entity_name\"],\n+        PgTestDataset(\"ihq_graph.stemmed_domain_named_entities\",\n+                      [\"stemmed_domain\", \"entity_name\", \"score\"],\n                       stemmed_domain_named_entities),\n         PgTestDataset(\"ihq_graph.wiki_brands\", [\"wiki_id\", \"default_label\", \"entity_name\"],\n                       wiki_brands)"
        },
        {
            "filename": "ihq-graph/resources/postgres/test/test_views.py",
            "patch": "@@ -27,11 +27,11 @@\n                     (\"Text Media\", \"en\", \"Text Media\"),\n                     (\"Blogs\", \"en\", \"Blogs\")]\n \n-# stemmed_domain, topic\n-domain_ihq_topics = [(\"shazam.com\", \"Music\"),\n-                     (\"kerrang.com\", \"Music Magazines\"),\n-                     (\"nationalgeographic.com\", \"Magazines\"),\n-                     (\"askamanager.org\", \"Blogs\")]\n+# stemmed_domain, topic, score\n+domain_ihq_topics = [(\"shazam.com\", \"Music\", 1.0),\n+                     (\"kerrang.com\", \"Music Magazines\", 0.7),\n+                     (\"nationalgeographic.com\", \"Magazines\", 0.4),\n+                     (\"askamanager.org\", \"Blogs\", 0.9)]\n \n # wiki_id, default_label, entity_name\n wiki_brands = [(111, \"Kerrang\", \"Kerrang.com\"),\n@@ -46,9 +46,9 @@\n named_entities = list({(x[2], x[2]) for x in wiki_brands})\n # entity_name, lang, label\n named_entity_labels = [(\"National Geographic Society\", \"en\", \"Nat. Geo.\")]\n-# stemmed_domain, entity_name\n-domain_named_entities = [(\"kerrang.com\", \"Kerrang.com\"),\n-                         (\"nationalgeographic.com\", \"National Geographic Society\")]\n+# stemmed_domain, entity_name, score\n+domain_named_entities = [(\"kerrang.com\", \"Kerrang.com\", 0.9),\n+                         (\"nationalgeographic.com\", \"National Geographic Society\", 0.5)]\n stemmed_domains = list({(x[0],) for x in domain_ihq_topics + domain_named_entities})\n \n content_categories = [(\"boring\", \"background noise\", 2, 4),\n@@ -73,8 +73,12 @@\n     PgTestDataset(\"ihq_graph.wiki_brands\", [\"wiki_id\", \"default_label\", \"entity_name\"], wiki_brands),\n     PgTestDataset(\"ihq_graph.wiki_brand_labels\", [\"wiki_id\", \"lang\", \"label\"], wiki_brand_labels),\n     PgTestDataset(\"ihq_graph.stemmed_domains\", [\"stemmed_domain\"], stemmed_domains),\n-    PgTestDataset(\"ihq_graph.stemmed_domain_ihq_topics\", [\"stemmed_domain\", \"topic_id\"], domain_ihq_topics),\n-    PgTestDataset(\"ihq_graph.stemmed_domain_named_entities\", [\"stemmed_domain\", \"entity_name\"], domain_named_entities),\n+    PgTestDataset(\"ihq_graph.stemmed_domain_ihq_topics\",\n+                  [\"stemmed_domain\", \"topic_id\", \"score\"],\n+                  domain_ihq_topics),\n+    PgTestDataset(\"ihq_graph.stemmed_domain_named_entities\",\n+                  [\"stemmed_domain\", \"entity_name\", \"score\"],\n+                  domain_named_entities),\n     PgTestDataset(\"ihq_graph.content_categories\",\n                   [\"content_type\", \"description\", \"block_level\", \"total_block_level\"],\n                   content_categories),\n@@ -90,9 +94,10 @@\n class TestCombinedIhqTopicsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedIhqTopicsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT topic_id, default_label, is_virtual from ihq_graph.ihq_topics ORDER BY topic_id\"\n@@ -106,7 +111,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for Blogs\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Blogs\", False, True)]\n         insert_rows(conn, \"client_graph.ihq_topics\", [\"topic_id\", \"is_virtual\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -139,7 +144,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_override_virtual(self, pg_conn):\n         # Override from True to False for Text Media\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Text Media\", False, False)]\n         insert_rows(conn, \"client_graph.ihq_topics\", [\"topic_id\", \"is_virtual\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -177,9 +182,10 @@ def test_override_virtual(self, pg_conn):\n class TestCombinedIhqTopicParentsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedIhqTopicParentsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT topic_id, parent_id FROM ihq_graph.ihq_topic_parents ORDER BY topic_id, parent_id\"\n@@ -193,7 +199,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for Music Magazines -> Music\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Music Magazines\", \"Music\", True)]\n         insert_rows(conn, \"client_graph.ihq_topic_parents\", [\"topic_id\", \"parent_id\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -232,7 +238,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_add_parent(self, pg_conn):\n         # Make Lifestyle a parent of Music\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Music\", \"Lifestyle\", False)]\n         insert_rows(conn, \"client_graph.ihq_topic_parents\", [\"topic_id\", \"parent_id\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -269,9 +275,10 @@ def test_add_parent(self, pg_conn):\n class TestCombinedIhqTopicLabelsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedIhqTopicLabelsView\"\n \n     def test_view_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT topic_id, lang, label FROM ihq_graph.ihq_topic_labels ORDER BY topic_id, lang\"\n@@ -285,7 +292,7 @@ def test_view_no_overrides(self, pg_conn):\n \n     def test_view_add_label(self, pg_conn):\n         # Add Spanish label for Lifestyle\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Lifestyle\", \"es\", \"Estilo de vida\")]\n         insert_rows(conn, \"client_graph.ihq_topic_labels\", [\"topic_id\", \"lang\", \"label\"], new_data)\n         with conn.cursor() as cursor:\n@@ -320,7 +327,7 @@ def test_view_add_label(self, pg_conn):\n \n     def test_change_label(self, pg_conn):\n         # Update the English label for Blogs\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Blogs\", \"en\", \"Bloggity Blog\")]\n         insert_rows(conn, \"client_graph.ihq_topic_labels\", [\"topic_id\", \"lang\", \"label\"], new_data)\n         with conn.cursor() as cursor:\n@@ -357,9 +364,10 @@ def test_change_label(self, pg_conn):\n class TestCombinedStemmedDomainIhqTopicsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedStemmedDomainIhqTopicsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\"\"\"\n                 SELECT stemmed_domain, topic_id \n@@ -379,7 +387,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for kerrang.com -> Music Magazines\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"kerrang.com\", \"Music Magazines\", True)]\n         insert_rows(conn, \"client_graph.stemmed_domain_ihq_topics\", [\"stemmed_domain\", \"topic_id\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -418,7 +426,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_add_mapping(self, pg_conn):\n         # Map kerrang.com directly to Music\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"kerrang.com\", \"Music\", False)]\n         insert_rows(conn, \"client_graph.stemmed_domain_ihq_topics\", [\"stemmed_domain\", \"topic_id\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -455,9 +463,10 @@ def test_add_mapping(self, pg_conn):\n class TestCombinedStemmedDomainsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedStemmedDomainsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\"\"\"\n                 SELECT stemmed_domain \n@@ -477,7 +486,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for kerrang.com\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"kerrang.com\", True)]\n         insert_rows(conn, \"client_graph.stemmed_domains\", [\"stemmed_domain\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -513,7 +522,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_add(self, pg_conn):\n         # Map kerrang.com directly to Music\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"bbc.com\", False)]\n         insert_rows(conn, \"client_graph.stemmed_domains\", [\"stemmed_domain\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -547,9 +556,10 @@ def test_add(self, pg_conn):\n class TestCombinedNamedEntitiesView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedNamedEntitiesView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT entity_name, default_label FROM ihq_graph.named_entities ORDER BY entity_name\"\n@@ -563,7 +573,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for Kerrang.com\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Kerrang.com\", \"Kerrang.com\", True)]\n         insert_rows(conn, \"client_graph.named_entities\", [\"entity_name\", \"default_label\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -599,7 +609,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_add(self, pg_conn):\n         # Create a new named entity\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Foo\", \"FooBar\", False)]\n         insert_rows(conn, \"client_graph.named_entities\", [\"entity_name\", \"default_label\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -633,9 +643,10 @@ def test_add(self, pg_conn):\n class TestCombinedNamedEntityLabelsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedNamedEntityLabelsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT entity_name, lang, label FROM ihq_graph.named_entity_labels ORDER BY entity_name, lang\"\n@@ -649,7 +660,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_add_label(self, pg_conn):\n         # Add Spanish label for National Geographic\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"National Geographic Society\", \"es\", \"National Geographic en Espa\u00f1ol\")]\n         insert_rows(conn, \"client_graph.named_entity_labels\", [\"entity_name\", \"lang\", \"label\"], new_data)\n         with conn.cursor() as cursor:\n@@ -684,7 +695,7 @@ def test_add_label(self, pg_conn):\n \n     def test_change_label(self, pg_conn):\n         # Update the English label for Kerrang.com\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = []\n         insert_rows(conn, \"client_graph.named_entity_labels\", [\"entity_name\", \"lang\", \"label\"], new_data)\n         with conn.cursor() as cursor:\n@@ -720,7 +731,7 @@ def test_change_label(self, pg_conn):\n     @pytest.mark.skip(\"Should we add an exclude flag for all the label tables? Or only allow overrides?\")\n     def test_exclude_label(self, pg_conn):\n         # Exclude the English label for Kerrang.com\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Kerrang.com\", \"en\", \"Kerrang.com\", True)]\n         insert_rows(conn, \"client_graph.named_entity_labels\", [\"entity_name\", \"lang\", \"label\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -761,9 +772,10 @@ def test_exclude_label(self, pg_conn):\n class TestCombinedWikiBrandsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedWikiBrandsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT wiki_id, default_label, entity_name FROM ihq_graph.wiki_brands ORDER BY wiki_id\"\n@@ -777,7 +789,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for Kerrang (111)\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(111, \"Kerrang\", True)]\n         insert_rows(conn, \"client_graph.wiki_brands\", [\"wiki_id\", \"default_label\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -813,7 +825,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_override_entity(self, pg_conn):\n         # Link National Geographic (TV) (333) to Kerrang instead of Nat Geo\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(333, \"Nat. Geo. TV\", \"Kerrang.com\", False)]\n         insert_rows(conn, \"client_graph.wiki_brands\", [\"wiki_id\", \"default_label\", \"entity_name\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -846,7 +858,7 @@ def test_override_entity(self, pg_conn):\n     @pytest.mark.skip(\"Should we drop the foreign key to the ihq_graph table and allow new wiki_ids?\")\n     def test_add(self, pg_conn):\n         # Create a new wiki brand\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = []\n         insert_rows(conn, \"client_graph.wiki_brands\", [\"wiki_id\", \"default_label\", \"exclude\"], new_data)\n         with conn.cursor() as cursor:\n@@ -880,9 +892,10 @@ def test_add(self, pg_conn):\n class TestCombinedWikiBrandLabelsView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedWikiBrandLabelsView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT wiki_id, lang, label FROM ihq_graph.wiki_brand_labels ORDER BY wiki_id, lang\"\n@@ -896,7 +909,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_add_label(self, pg_conn):\n         # Add Spanish label for National Geographic (222)\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(222, \"es\", \"National Geographic en Espa\u00f1ol\")]\n         insert_rows(conn, \"client_graph.wiki_brand_labels\", [\"wiki_id\", \"lang\", \"label\"], new_data)\n         with conn.cursor() as cursor:\n@@ -931,7 +944,7 @@ def test_add_label(self, pg_conn):\n \n     def test_change_label(self, pg_conn):\n         # Update the English label for Kerrang.com (111)\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(111, \"en\", \"Something Else\")]\n         insert_rows(conn, \"client_graph.wiki_brand_labels\", [\"wiki_id\", \"lang\", \"label\"], new_data)\n         with conn.cursor() as cursor:\n@@ -968,9 +981,10 @@ def test_change_label(self, pg_conn):\n class TestCombinedStemmedDomainNamedEntitiesView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestCombinedStemmedDomainNamedEntitiesView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\"\"\"\n                 SELECT stemmed_domain, entity_name \n@@ -990,7 +1004,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True for kerrang.com -> Music Magazines\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"kerrang.com\", \"Kerrang.com\", True)]\n         insert_rows(conn,\n                     \"client_graph.stemmed_domain_named_entities\",\n@@ -1032,7 +1046,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_add_mapping(self, pg_conn):\n         # Map kerrang.com directly to National Geographic\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"kerrang.com\", \"National Geographic Society\", False)]\n         insert_rows(conn,\n                     \"client_graph.stemmed_domain_named_entities\",\n@@ -1071,9 +1085,10 @@ def test_add_mapping(self, pg_conn):\n \n class TestCombinedContentCategoriesView(PgTestHelper):\n     datasets = shared_datasets\n+    application_name = \"TestCombinedContentCategoriesView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT content_type, description, block_level, total_block_level \"\n@@ -1089,7 +1104,7 @@ def test_no_overrides(self, pg_conn):\n \n     def test_add_content_type(self, pg_conn):\n         # Create a new content type\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Foo\", \"FooBar\", 1, 1)]\n         insert_rows(conn,\n                     \"client_graph.content_categories\",\n@@ -1124,7 +1139,7 @@ def test_add_content_type(self, pg_conn):\n \n     def test_override_reduce_levels(self, pg_conn):\n         # Override the block and total block levels for a content type with *lower* values\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"boring\", \"background noise\", 1, 1)]\n         insert_rows(conn,\n                     \"client_graph.content_categories\",\n@@ -1159,7 +1174,7 @@ def test_override_reduce_levels(self, pg_conn):\n \n     def test_override_raise_block_level(self, pg_conn):\n         # Override the block level a higher value\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"boring\", \"background noise\", 3, 3)]\n         insert_rows(conn,\n                     \"client_graph.content_categories\",\n@@ -1196,7 +1211,7 @@ def test_cannot_raise_total_block_level(self, pg_conn):\n         # Try to override the total block level of 'boring' (4) with a higher value (5) - this will be ignored\n         # This prevents a 'total block' configured by IHQ being raised by a client, allowing activity with\n         # higher classifications through\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"boring\", \"background noise\", 5, 5)]\n         expected_data = [(\"boring\", \"background noise\", 5, 4)]\n         insert_rows(conn,\n@@ -1234,9 +1249,10 @@ def test_cannot_raise_total_block_level(self, pg_conn):\n class TestIhqTopicContentCategoriesView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestIhqTopicContentCategoriesView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT topic_id, content_type, notes, content_level \"\n@@ -1253,7 +1269,7 @@ def test_no_overrides(self, pg_conn):\n     def test_increase_content_level(self, pg_conn):\n         # Override the level classification for a topic with a higher value\n         # This allows a client override to rate something more strongly than our default\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Blogs\", \"boring\", \"we are really not interested\", 3, False)]\n         insert_rows(conn,\n                     \"client_graph.ihq_topic_content_categories\",\n@@ -1294,7 +1310,7 @@ def test_cannot_decrease_content_level_below_total_block(self, pg_conn):\n         # \"total block\" level for that category, with a value below that level - the view should return the\n         # 'total block' instead\n         # This prevents a client override from allowing content IHQ has classified as \"completely disallowed\" through\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"NSFW\", \"naughty\", \"we're a bit lax on these things\", 2, False)]\n         expected_data = [(\"NSFW\", \"naughty\", \"we're a bit lax on these things\", 3, False)]\n         insert_rows(conn,\n@@ -1333,7 +1349,7 @@ def test_cannot_decrease_content_level_below_total_block(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True on Blogs-boring\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Blogs\", \"boring\", \"we are really not interested\", 1, True)]\n         insert_rows(conn,\n                     \"client_graph.ihq_topic_content_categories\",\n@@ -1375,7 +1391,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_cannot_exclude_classification_above_total_block_level(self, pg_conn):\n         # Set exclude = True on Blogs-boring\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"CDN\", \"boring\", \"try to remove this\", 4, True)]\n         expected_data = [(\"CDN\", \"boring\", \"try to remove this\", 4, False)]\n         insert_rows(conn,\n@@ -1420,9 +1436,10 @@ def test_cannot_exclude_classification_above_total_block_level(self, pg_conn):\n class TestNamedEntityContentCategoriesView(PgTestHelper):\n \n     datasets = shared_datasets\n+    application_name = \"TestNamedEntityContentCategoriesView\"\n \n     def test_no_overrides(self, pg_conn):\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         with conn.cursor() as cursor:\n             base_data = cursor.execute(\n                 \"SELECT entity_name, content_type, notes, content_level \"\n@@ -1439,7 +1456,7 @@ def test_no_overrides(self, pg_conn):\n     def test_increase_content_level(self, pg_conn):\n         # Override the level classification for a topic with a higher value\n         # This allows a client override to rate something more strongly than our default\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"National Geographic Society\", \"boring\", \"we are really not interested\", 3, False)]\n         insert_rows(conn,\n                     \"client_graph.named_entity_content_categories\",\n@@ -1480,7 +1497,7 @@ def test_cannot_decrease_content_level_below_total_block(self, pg_conn):\n         # \"total block\" level for that category, with a value below that level - the view should return the\n         # 'total block' instead\n         # This prevents a client override from allowing content IHQ has classified as \"completely disallowed\" through\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Naughty Pictures\", \"naughty\", \"we're a bit lax on these things\", 2, False)]\n         expected_data = [(\"Naughty Pictures\", \"naughty\", \"we're a bit lax on these things\", 3, False)]\n         insert_rows(conn,\n@@ -1519,7 +1536,7 @@ def test_cannot_decrease_content_level_below_total_block(self, pg_conn):\n \n     def test_override_exclude(self, pg_conn):\n         # Set exclude = True on \"National Geographic Society\"-boring\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"National Geographic Society\", \"boring\", \"we are really not interested\", 1, True)]\n         insert_rows(conn,\n                     \"client_graph.named_entity_content_categories\",\n@@ -1561,7 +1578,7 @@ def test_override_exclude(self, pg_conn):\n \n     def test_cannot_exclude_classification_above_total_block_level(self, pg_conn):\n         # Set exclude = True on \"Doubleclick ad network\"-boring\n-        conn = connect(pg_conn)\n+        conn = connect(pg_conn, self.application_name)\n         new_data = [(\"Doubleclick ad network\", \"boring\", \"try to remove this\", 4, True)]\n         expected_data = [(\"Doubleclick ad network\", \"boring\", \"try to remove this\", 4, False)]\n         insert_rows(conn,"
        }
    ],
    "reviews": [
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "geo-harrison",
            "state": "COMMENTED",
            "body": "The set up looks great! I've left a few comments questions before approving\r\n\r\nAlso, I had a couple other questions:\r\n- Are we planning to soft delete entries? It might be worth adding in a `deleted_at` and/or `deleted_by` to the tables to make any changes easily reversible \r\n- Are we going to store actual audit changes in an audit table to see e.g. to see label values before and after modification? Again for logging and reversibility purposes, or is this too much work for v1?"
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "cdagraca",
            "state": "COMMENTED",
            "body": ""
        },
        {
            "user": "geo-harrison",
            "state": "DISMISSED",
            "body": "Happy to approve after our catch up. On the point of where the confidences will be stored I'm leaning towards storing them client side in case we need them for any client specific projects / tasks, but confirming with other team members."
        },
        {
            "user": "geo-harrison",
            "state": "APPROVED",
            "body": ""
        }
    ],
    "linked_issues": "AIPROD-1864 Add audit columns and triggers to graph",
    "readme": "# ML engineering repository.\n\nRepository that contains the code for refreshing the data. This includes three categories:\n\n1. Orchestration\n2. PySpark jobs\n2. Python Steps\n\n\n# Notes for developers.\n\nAfter cloning the repo, developers should install the git hooks for working with this repo.\nFrom the root directory of the repo, run the following command.\n```bash\ncp _utils/hooks/commit-msg .git/hooks/commit-msg; chmod 755 .git/hooks/commit-msg\n```\n\nCodebase maintainers recommend using a venv for working in this repository. \nAlso, pycharm is the suggested IDE.\n\nIn the link below, users can find information on how to set up a venv in pycharm.\nhttps://www.jetbrains.com/help/pycharm/creating-virtual-environment.html\n\n# Working with the repo.\n\n## Code traceability.\n\nFor having better understanding of how our software evolves, it is important that developers reference the ticket number in\nthe commit message.\ni.e each commit should include the Jira ticket ID. e.g [#CDS-123]\n\n\n## Working with orchestration module\n\nFor working with this repo locally, developers will need to run these commands in order to install the requirements of the solution.\n\nYour local environment will require packages to be installed that are either installed in the Airflow environment by default,\nor are required only for testing - these are in the `local_requirements.txt` files.\n\nIHQ's own packages need to be downloaded, added to `plugins.zip` for deployment, and referenced by local path in the Airflow `requirements.txt` -\nthese are in the `internal_requirements.txt`\n\nFinally, we have external packages not installed by default on Airflow, that just need to be referenced \nin the requirements file in the normal way - these are in the `external_requirements.txt` files. \n\nThe deployment process packages and references the internal and external dependencies, and ignores the local ones.\nFor local operation, all three are needed and can just be applied separately \n\n```bash\npip3 install -r common/orchestration/local_requirements.txt\npip3 install -r common/orchestration/external_requirements.txt\npip3 install -r common/orchestration/internal_requirements.txt --extra-index-url https://pip.intenthq.com/simple/\npip3 install -r <<client>>/orchestration/local_requirements.txt\npip3 install -r <<client>>/orchestration/external_requirements.txt\npip3 install -r <<client>>/orchestration/internal_requirements.txt --extra-index-url https://pip.intenthq.com/simple/\n```\n#### Airflow development guidelines.\n\nFor developing airflow, it is important to separate the logic of the orchestration vs any other logic, \nsuch as calculating variables, branching logic, etc.\n\n##### Creating DAGs.\n\nA DAG (Directed Acyclic Graph) is the core concept of Airflow, collecting Tasks together, \norganized with dependencies and relationships to say how they should run.\n\nA dag should be defined in a python file. It must be seen as a config file for defining the different steps that need to happen in the pipeline.\nLogic about branching, setting up variables, calculations or any other kind of logic should go in a separate utils file as\nit is likely that code can be used in multiple dags.\n\nLogic that is specific for the client should go under `<<client name>>/orchestration/utils`. \nLogic that is client agnostic should go in `common/orchestration/utils`\n\n\n##### Creating Branching logic\n\nBranching can be complex. Our pipelines depend on multiple conditions such as date of the refresh, \nfiles being present in a bucket, other variables, etc.\n\nFor implementing that, developers should create this logic in a separated way, probably using the client specific utilities file.\n\n```bash\ncbl/orchestration/utils/cbl_dag_utils.py\n...\n\n```\n\n##### Setting up variables\n\nVariables are used internally as the shared dagrun configuration. \nthe scope of the variables is global, and they can be accessed by multiple dags and users at the same time.\n\nMost of our dags follow a pattern where these variables are calculated at the beginning of the DAG, and\nare used during the execution of the DAG. Variables should be used for statuses of the DAG, such as dates, folders, name of buckets etc. \nThe main advantage of using variables is that users can access edit and modify the values from the UI.\n\n\nAirflow DAGs, should define the name of the variables that that DAG will use. It can also specify a default value.\nDevelopers must use \n\n```\nrequired_variables = {\n    \"variable1\": \"default1\",\n    \"variable2\": \"default2\"\n}\n\ncommon.orchestration.utils.airflow_variable_utils.define_airflow_variables_with_defaults(required_variables)\n```\n\n##### Using XCOMs\n\nXCOMs are a useful feature of airflow, they are used for exchaging information across tasks.\nsee. https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html\n\nUnlike variables, XCOMs should be used for the kind of information that is not relevant for the user that is using airflow. \nThe values in this kind of communication are not directly accessible by users. Also, XCOMS are dependent of the dagrun.\n\n##### Adding plugins\n\nDevelopers should think about airflow development as a \"set of capabilities\" rather than a bespoke solution for each customer.\n\nWhen implementing code for interacting with other service \n(such as ingestion, s3 bucket, platform-status) developers should write a plugin that later can be user for another client.\n\nSee. \n```\ncommon/orchestration/utils/sensors\ncommon/orchestration/utils/operators\n```\n\n\nFor more information about airflow, check our coda document. \nhttps://coda.io/d/Data-Science-and-ML-Eng_dK5h4iVEEUo/Airflow-Guidelines_suZvI#_lu0e3\n\n### Set up local airflow dev environment.\n\n\n1. If you haven't already, install the Opal command-line library: `brew install opalsecurity/brew/opal-security`\n2. Run `export CLIENT=<<client_name>>` env variable, Select from `cbl`, or `vzw`.\n3. Run `_local_build/generate_release_bundle.sh`\n4. You should only need to do this the first time on any machine\n5. You need to be connected to the VPN when running this, in order for it to be able to download the packages from our IHQ repo\n6. Build the custom ihq-airflow image `docker build . --tag ihq-airflow:2.2.2`\n7. Export the image name to the env variable. `export AIRFLOW_IMAGE_NAME=ihq-airflow:2.2.2`\n8. Check that all the environment variables in `.env` and `.envrc` are well-defined\n9. Export AWS variables: `export AWS_FOLDER=~/.aws`\n10. Be sure that `AWS_DEFAULT_REGION` in the docker compose has the correct region name\n11. Log in the specific aws account you would like to refer with `opal login --email <you>@intenthq.com` and `opal iam-roles:start`\n    1. You will need to request access to each role you require (e.g. VZW-Engineer) vial the Opal web console - assuming you already have the AWS permissions in place, approval is very quick \n12. Run the following command from the terminal: `docker compose up -d` \n   (without the `-d` it runs in the foreground and spams your terminal with competing log messages from about 6 different containers)\n   With this configuration, you will be allowed to build any doc in local, and save them in the `${CLIENT}/orchestration/dags` folder.\n13. To shut down, run `docker compose down -v` (without the `-v` it leaves orphaned volumes and you'll quickly run out of space)\n\n\n## Working with python_steps module\n\n`python_steps` is a module that contains the code for performing the parts of the refresh that doesn't need to be distributed.\nThey are intended to be published as packages that developers can import into their dags code.\n\nFor working with this repo locally, developers will need to run this command in order to install the dependencies of the package.\n```bash\n  pip install -e <<client_name>>/python_steps --extra-index-url https://pip.intenthq.com/simple/\n```\n\n\n## Working with pyspark_jobs module\n\n`pyspak_jobs` is a module that contains the code for performing the parts of the refresh that need to be distributed across multiple\nnodes due to the high amount of data that they process. They are intended to be deployed to a s3 bucket and executed via EMR.\n\nFor working with this module locally, developers will need to run this command in order to install the dependencies of the application.\n\n```bash\npip install pyspark\n```\n\nAlso, depending on the job, some other packages might be needed as well. \nThe full list of required packages for each job is listed in the airflow code that submits the job to EMR.\n\n## Unit testing\n\nAll the code in this repository has to be unit tested.\nNew unit tests should follow these guidelines. \n\n- https://www.educative.io/blog/unit-testing-best-practices-overview\n- https://coda.io/d/Data-Science-and-ML-Eng_dK5h4iVEEUo/Testing-Guidelines_sue6a\n\nBefore pushing the code to master, developers should run the whole test suite of the client and module they are working on.\n\n`python -m unittest discover <<CLIENT>>/<<MODULE>>/test`\n\nThese unit test suites will also be executed as Github actions when a pull request is created.\n\nFor `vzw/pyspark_jobs/test` some additional path setting is required, because the way we upload the files to the s3 `emr_code`\nlocation doesn't follow the same folder structure as the repo, so the package paths in the imports are wrong. Not all\nof the following are needed for every test, but this will cover the requirements for everything.\n\n`PYTHONPATH=\"${PYTHONPATH}:common/common/utils:common/pyspark_jobs:vzw/common/utils:vzw/pyspark_jobs:vzw/pyspark_jobs/smac\" python3 -m unittest <<discover|test path>>`\n\n# Integration\n- In progress.\n\n# Releasing packages.\nAll the python packages will be released using Github actions. For doing this, users will navigate to https://github.com/intenthq/mle/actions and\nuse the `Release python steps package` action. \n\nWhen triggered, an input form will appear, the user should fill the following values.\n\n- Use workflow from. This is the branch to release from.\n- Name of the client. Name of the python step package to be released (cbl, common, etc.)\n- Version of the package to be released (X.X.X) (Semantic version of the package to be released. https://semver.org/)\n\nAfter completion of the job, packages will be published in https://pip.intenthq.com/simple/, available company-wide.\n\n\n# Deployment.\nAll the code will be deployed using Github actions. For doing this, users will navigate to https://github.com/intenthq/mle/actions and\nuse the `Deploy spark jobs code.` or `Deploy orchestration code.` action.\n\n## Deploy orchestration code. (Airflow)\n\nTrigger the `Deploy orchestration code.` action and fill the form.\n\n- Use workflow from. This is the branch to deploy from.\n- Name of the client where airflow will be deployed.\n- Region to deploy. AWS region the client belongs to.\n\nThis action will generate a release bundle with all the python dependencies and create a requirements file listing the contents\nof the release bundle. It will also upload the code for the selected client + the common code and upload it to the `XXXXXXX-CLIENT-airflow` bucket,\nmirroring the same folder structure existing in the repository.\n\n## Deploy refresh code. (Pyspark)\n\nTrigger the `Deploy spark jobs code.` action and fill the form.\n\n- Use workflow from. This is the branch to deploy from.\n- Name of the client where airflow will be deployed.\n- Region to deploy. AWS region the client belongs to.\n\n\nThis action will upload the pyspark code from.\n```\ncommon/pyspark_jobs #If existent\nCLIENT/pyspark_jobs\n```\n\nTo the s3 bucket defined in the `.github/workflows/_helpers/deployment_locations.json` file.\n\n# Code owners\n\nThe following people have been named as code-owners for the different modules of the repository.\nCode owners will be added as default reviewers in the PR when the code of the corresponding folder is modified.\n\n- `/common/` @manuel-fidalgo-fierro\n- `/cbl/` @SriramReddyMuli @efgeorge18\n- `/vzw/` @cdagraca @deepakihq\n"
}