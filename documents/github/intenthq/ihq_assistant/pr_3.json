{
  "title": "strip coda images; make callable vector functions",
  "description": null,
  "author": "cdagraca",
  "created_at": "2025-03-04T14:10:22Z",
  "changed_files": [
    {
      "filename": "db/coda_linear_github_embeddings.db/index.faiss",
      "patch": ""
    },
    {
      "filename": "db/coda_linear_github_embeddings.db/index.pkl",
      "patch": ""
    },
    {
      "filename": "load_vector_db.py",
      "patch": "@@ -1,28 +1,29 @@\n-import os\n-\n from faiss import IndexFlatL2\n from langchain import hub\n from langchain.chat_models import init_chat_model\n from langchain_community.docstore.in_memory import InMemoryDocstore\n from langchain_community.vectorstores import FAISS\n from langchain.embeddings import OpenAIEmbeddings\n \n-os.environ[\"OPENAI_API_KEY\"] = \"<your key>\"\n-\n-embeddings = OpenAIEmbeddings()\n-faiss = FAISS(embedding_function=embeddings, index=IndexFlatL2, docstore=InMemoryDocstore(), index_to_docstore_id={})\n-db = faiss.load_local(folder_path=\"./db/coda_linear_embeddings.db\",\n-                      embeddings=embeddings,\n-                      allow_dangerous_deserialization=True)\n-\n-llm = init_chat_model(\"gpt-4\", model_provider=\"openai\")\n-\n-question = \"What is the IHQ Graph\"\n-\n-docs = db.similarity_search(query=question, k=3)\n-docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n-\n-prompt = hub.pull(\"rlm/rag-prompt\")\n \n-response = llm.invoke(prompt.invoke({\"question\": question, \"context\": docs_content}))\n-print(response.content)\n+# These functions assume os.environ[\"OPENAI_API_KEY\"] is set\n+def load_embeddings(source_path:str = \"coda_linear_github_embeddings.db\"):\n+    embeddings = OpenAIEmbeddings()\n+    faiss = FAISS(embedding_function=embeddings,\n+                  index=IndexFlatL2,\n+                  docstore=InMemoryDocstore(),\n+                  index_to_docstore_id={})\n+    return faiss.load_local(folder_path=f\"./db/{source_path}\",\n+                            embeddings=embeddings,\n+                            allow_dangerous_deserialization=True)\n+\n+\n+def execute_query(question: str, embeddings:list = None, model:str = \"gpt-4\"):\n+    if embeddings is not None and len(embeddings) > 0:\n+        docs = embeddings.similarity_search(query=question, k=3)\n+        docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n+    else:\n+        docs_content = \"\"\n+    prompt = hub.pull(\"rlm/rag-prompt\")\n+    llm = init_chat_model(model, model_provider=\"openai\")\n+    return llm.invoke(prompt.invoke({\"question\": question, \"context\": docs_content})).content"
    },
    {
      "filename": "vectorise_data.py",
      "patch": "@@ -1,4 +1,5 @@\n import os\n+import re\n \n from langchain.document_loaders import TextLoader\n from langchain.text_splitter import CharacterTextSplitter\n@@ -17,6 +18,10 @@\n     for file in os.listdir(f\"{file_base_path}/{folder}\"):\n         loader = TextLoader(file_path=f\"{file_base_path}/{folder}/{file}\", encoding=\"utf-8\")\n         data = loader.load()\n+        if folder == \"coda\":\n+            # Strip out the encoded images\n+            for d in data:\n+                d.page_content = re.sub(\"(\\[|\\()data:image.+(\\]|\\))\", \"\", d.page_content)\n         docs += text_splitter.split_documents(data)\n \n embeddings = OpenAIEmbeddings()"
    }
  ],
  "reviews": [
    {
      "user": "javierpedreira",
      "state": "APPROVED",
      "body": ""
    }
  ],
  "linked_issues": "No linked issues",
  "readme": "# Install dependencies\n\nRun\n\n```\npip3 install -r ./requirements.txt\n```\n\n# Start the bot\n\nCopy `.env.template` to `.env`, fill the tokens with real tokens\n\nCreate venv\n\n```\npython3 -m venv venv\nsource venv/bin/activate\n```\n\nRun\n\n```\npython3 slack_faiss_bot.py\n```\n"
}