# Problem Definition and Purpose

- We are generating embeddings for three different samples. They are for refreshes (13 weeks) ending on:
  - July 7th
  - July 14th
  - July 21st
- We want to determine whether those datasets are different or if they are virtually the same.

# Background and Motivation

- The intersection of data across those datasets is large, since they share most of the data (~11 weeks). 
- We need to determine how different those datasets are, and hopefully quantify that difference.

# Data Sources

- We are using the **embeddings** generated by the MLE team. They are available in the following paths:
  - *"s3://226109243659-vzw-data-science-longterm/cdagraca/lake/refreshes/refresh_20240707/sampled_embeddings/"*
  - *"s3://226109243659-vzw-data-science-longterm/cdagraca/lake/refreshes/refresh_20240714/sampled_embeddings/"*
  - *"s3://226109243659-vzw-data-science-longterm/cdagraca/lake/refreshes/refresh_20240721/sampled_embeddings/"*
- Each of those paths contains five different samples of the corresponding embeddings.
- We also used the **means**, available in:
  - s3://226109243659-vzw-data-store/lake/refreshes/refresh_20240725/churn_inference_20240707.delta/
  - s3://226109243659-vzw-data-store/lake/refreshes/refresh_20240725/churn_inference_20240714.delta/
  - s3://226109243659-vzw-data-store/lake/refreshes/refresh_20240725/churn_inference_20240721.delta/

# Exploratory Data Analysis

Initially, I compared the embeddings of some single users using different comparison methods. I took inspiration from this [Article](https://www.pinecone.io/learn/vector-similarity/) ran the comparisons using a two different embedding samples for the same user and the same refresh. I calculated the metrics for the pair of different embedding and also for the same embedding (basically, AB and AA).

### Euclidean distance







### Cosine similarity







### Dot product







The Euclidean distance is 0 when both embeddings are the same, and it grows with how different the embeddings are. The cosine similarity will vary between 0 and 1, where 1 means exactly the same vectors. The dot product is the sum of the products of the elements, so it is more difficult to interpret.

Initially, I used Euclidean distance, as it's the more easily interpretable of the three. However, after looking into the embeddings more deeply, we decided to use cosine similarity. This is due to the significant variance in the values of the embeddings, which could make the Euclidean distance not the best measure.



### Embedding samplings for same user



Each of the first three sections of results shows different samplings for a same user. The last section on the bottom shows comparison across different users.

On the one hand, the similarity between different users is lower than the similarity between same users, which is good and expected.

However, what I find interesting is that there isn't much variance between the distance across user samples. For instance, in the first group, the distance is always around 0.37-0.39. 

# Experiment Results

## Results with Euclidean Distance

I use samples of around 660k users (1% of the 66M users available).

Firstly, I compared one of the embeddings dataset (Jul 7) to a **randomly** generated embeddings, so that we can have a baseline:







*Note: the histogram above (Distances between Jul 7 and random) does not follow the scale of the other histograms, as its values are too different from the rest of the histograms.*



Secondly, I compared **two different samplings of the same users for the same refresh**. I wanted to check what is an expected variation.







The euclidean distance between samples of the same user in the same refresh is on average **13.79**.



Then, I compared the **user embeddings across the** **different refreshes** described in the beginning of the document:



















The means varied from 15.54 to 16.82. When compared to the previous mean of 13.79, they are all higher than that. When we compare other metrics (min, max, Qs) they are also higher across refreshes.



### Comparing means

I also compared the differences the refreshes using the means instead of the z scores. That way, we can remove the random component and have a more stable representation of the differences week on week.

Here are the **user means across different refreshes.**













## Results with Cosine Similarity

I used samples of around 66k users (0.1% of the 66M users available).

Firstly, I compared one of the embeddings dataset (Jul 7) to a **randomly** generated embeddings, so that we can have a baseline:









Then, I compared **two different sampled embeddings of the same users for the same refresh**. I wanted to check what kind of variation is due to the randomness added to the model.













The cosine similarity between samples of the same user in the same refresh is on average **0.74**.

One important observation here is that we have user with low similarity (results go down to zero similarity in some cases, which means orthogonal vectors). That could indicate that the embeddings are not very useful for some users, as different sampling of those embeddings are very different among themselves.



Then, I compared the **user embeddings across the three different refreshes**:



























The means varied from 0.66 to 0.70. When compared to the previous mean of 0.74, they are all lower than that. When we compare other metrics (min, max, Qs) they are also lower across refreshes. That indicates that there's more differences between the week-on-week data than different sampling of same-week data. 

One important observation here is that we have user with low similarity (results go down to zero similarity in some cases, which means orthogonal vectors).



### Comparing means

I also compared the differences in the refreshes using the *means* instead of the *z scores*. That way, we can remove the random component added by the network and have a more stable representation of the differences week on week.

Here are the **user means across different refreshes.**















Here we as many cases of low similarity as we did when we compared z-scores. It seems that for some users, the z-scores are very sensitive to the random value added. The means seem to provide higher differences since they are not subjected to the same volatility that the z-scores are.



## **Analysing Embedding values**

For each of the 512 features, you can find info about their corresponding distribution in the columns below.

https://docs.google.com/spreadsheets/d/1O1T70CEsTHaAjSgLzW6fd9e3izYm3f3eS_zqtzeUpTk/edit



For an aggregated and visual representation, you can look at the following histograms, which show the distributions of mean, min, max, standard deviation and the difference between max and min, aggregated across all the features.

















It seems that the feature values vary across a wide range - the difference between max and min is roughly between 4 and 15, for instance. So we are probably better off using cosine similarity instead of the euclidean distance, so we are less affected by features with a high value (i.e. magnitude in the vector).

# Overall Conclusions and Outcomes

- **We have found differences across the three different sets of embeddings.**
  - They are different when compared against each other, and they show indications of a week-on-week movement across the three weeks. That shows we're capturing different behaviour in each of the weeks.
  - The amount of their difference is also bigger than the difference between different embedding samples for the same time period. That indicates the difference between the different sets does not only come from the randomness added to the embeddings. 
- **We have also** **found differences** across the **means** of the three different sets of embeddings.
  - Comparing means allows us to control for the random component, which gives us more confidence that differences between the three different sets is coming from data differences, and not randomness.
- The means provide more stable embeddings with clearer differences across the weeks. For that reason, **we have decided to send means to Verizon** instead of the z scores.
  - The means are arguably closer to the input and could potentially provide less privacy. However, we believe it is safe to use the means, as (1) the people receiving these embeddings do not have access to our network and (2) the means should provide a high level of privacy anyway, as they generated by the overall training of the full network.

# 

# Future Work 

- Compare the means across refreshes (on top of the embeddings) DONE
- Run feature profiling on a refresh (see how much features vary across users) DONE
- Feature profiling across refreshes DEPRIORITISED
- Use new measure using percentage diff across features DEPRIORITISED
- Find an appropriate statistical test DEPRIORITISED