When we built the current best model, we trained on the November refresh. We assessed that in December and January data, which we presented at the dev gate.

Now, we used the same model to infer on February data. The high level results for user and brand level are below.









The results are overall pretty comparable, having decreased slightly in the February results. If we look at the f1 scores, they **decreased** about **0.5% for user** metrics and **0.1% for brand** metrics. This is exactly what we expected and hoped. That is, that there would be a decrease, since we are predicting data three months in the future (February/2024) using a model trained in November/2023, but that the results would still be similar.



# Visualisations

Here are some visualisations to help interpret the results better.

## Brand Frequency

This is the frequency of the number of brands across users. That is, on *x* you have the number of brands and in *y* you have the frequency.

### In the input:

The input here means the actual refresh. That is, the full data of three months without any filters. 







  


*For the histogram, I filtered out counts above* 1000*, as they are mainly 1s and would not show in the graph anyway. The number of filtered out counts is* 721 *(out of ~1M). The table and the boxplot show all counts, including the ones above* 1000*.*



### In the target

The target is the data we are assessing the model against. That corresponds to one month of data, and it includes the filters for minimum interaction (more than one interaction on a day, more than one day in a month).









*For the histogram, I filtered out counts above 400, as they are mainly 1s and would not show in the graph anyway. The number of filtered out counts is 417 (out of ~1M). The table and the boxplot show all counts, including the ones above 400.*

### In the model predictions:

Those are the predictions generated by the model.











*For the histogram, I filtered out counts above 400, as they are mainly 1s and would not show in the graph anyway. The number of filtered out counts is 1097 (out of ~1M). The table and the boxplot show all counts, including the ones above 400.*



## Score distribution for low and high volume brands

I selected a few brands with a high and low volume of data. I simply listed them ordered by volume and selected a few that I thought could be relevant from the top and the bottom. I did not change that selection after I produced the plots. For reference, here's some info about the selected brands

| Brand | Volume | f1 score |
| --- | --- | --- |
| Entertainment Tonight Canada | 996 | 0 |
| Snapple | 1,011 | 0 |
| Beyond Meat | 1,013 | 0 |
| OkCupid | 1,022 | 0.02 |
| Pepco | 1,188 | 0 |
| Coco Chanel | 1,202 | 0.07 |
| CNN | 100,233 | 0.46 |
| DoorDash | 143,746 | 0.44 |
| Uber | 196,501 | 0.55 |
| ESPN | 211,543 | 0.62 |
| Netflix | 269,913 | 0.51 |
| Walmart | 304,322 | 0.61 |


For each brand, I collected the scores for all users in a 1M sample and plotted them in a histogram. Results are below.

























We can see that in the high volume brands, the distribution is closer to a normal distribution. In the low volume brands, the curves are very similar to each other and could be potentially interpreted as random. The takeaway here in my opinion is that low-volume brands do not carry enough insight to produce meaningful brands.