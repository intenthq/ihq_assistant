{
  "title": "choose one specific embeddings source to use as context",
  "description": null,
  "author": "cdagraca",
  "created_at": "2025-03-04T18:29:31Z",
  "changed_files": [
    {
      "filename": "rag_util_functions.py",
      "patch": "@@ -6,6 +6,12 @@\n from langchain.embeddings import OpenAIEmbeddings\n from typing import Optional\n \n+source_db_map = {\n+    \"linear\": \"linear_embeddings.db\",\n+    \"github\": \"github_embeddings.db\",\n+    \"coda\": \"coda_embeddings.db\"\n+}\n+\n \n # These functions assume os.environ[\"OPENAI_API_KEY\"] is set\n def load_embeddings(source_path: str = \"coda_linear_github_embeddings.db\"):\n@@ -19,7 +25,29 @@ def load_embeddings(source_path: str = \"coda_linear_github_embeddings.db\"):\n                             allow_dangerous_deserialization=True)\n \n \n-def execute_query(question: str, embeddings_db: Optional[FAISS] = None, model: str = \"gpt-4\"):\n+def load_databases():\n+    dbs = {}\n+    embeddings = OpenAIEmbeddings()\n+    faiss = FAISS(embedding_function=embeddings,\n+                  index=IndexFlatL2,\n+                  docstore=InMemoryDocstore(),\n+                  index_to_docstore_id={})\n+    for source in source_db_map.keys():\n+        dbs[source] = faiss.load_local(folder_path=f\"./db/{source_db_map[source]}\",\n+                                       embeddings=embeddings,\n+                                       allow_dangerous_deserialization=True)\n+    return dbs\n+\n+\n+def rank_sources(question: str, model: str = \"gpt-4o\"):\n+    llm = init_chat_model(model, model_provider=\"openai\")\n+    result = llm.invoke(\n+        \"I have 3 data sources: coda, linear and github. Produce a comma-separated list ordering these from most to \"\n+        f\"least relevant for answering the following question: {question}\")\n+    return [r.strip() for r in result.content.lower().split(\",\")]\n+\n+\n+def execute_query(question: str, embeddings_db: Optional[FAISS] = None, model: str = \"gpt-4o\"):\n     if embeddings_db is not None:\n         docs = embeddings_db.similarity_search(query=question, k=3)\n         docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n@@ -28,3 +56,9 @@ def execute_query(question: str, embeddings_db: Optional[FAISS] = None, model: s\n     prompt = hub.pull(\"rlm/rag-prompt\")\n     llm = init_chat_model(model, model_provider=\"openai\")\n     return llm.invoke(prompt.invoke({\"question\": question, \"context\": docs_content})).content\n+\n+\n+def query_preferred_source(question: str, model: str = \"gpt-4o\"):\n+    source_dbs = load_databases()\n+    db = source_dbs[rank_sources(question)[0]]\n+    return execute_query(question, db, model)\n\\ No newline at end of file"
    }
  ],
  "reviews": [
    {
      "user": "igor-pechersky",
      "state": "COMMENTED",
      "body": "why gpt-4?\r\ngpt-4: The standard GPT-4 model has a context window of 8,192 tokens. â€‹\r\ngpt-4o: An enhanced version known as GPT-4 Turbo offers a significantly larger context window of 128,000 tokens."
    }
  ],
  "linked_issues": "No linked issues",
  "readme": "# Install dependencies\n\nRun\n\n```\npip3 install -r ./requirements.txt\n```\n\n# Start the bot\n\nCopy `.env.template` to `.env`, fill the tokens with real tokens\n\nCreate venv\n\n```\npython3 -m venv venv\nsource venv/bin/activate\n```\n\nRun\n\n```\npython3 slack_faiss_bot.py\n```\n"
}